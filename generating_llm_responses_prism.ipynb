{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Vanilla LLM Responses\n",
    "### We're going to generate responses from a LLM for each question, based on the prism dataset.\n",
    "\n",
    "The input is the prism dataset.\n",
    "The output is a large CSV of questions, opinions, and LLM responses. LLM responses & questions are 1-to-1 but duplicated across varying opinions. This is not space efficient but makes it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/HannahRoseKirk/prism-alignment/conversations.jsonl\", lines=True)\n",
    "df = df[df['conversation_type']!='unguided']\n",
    "df.rename(columns={'opening_prompt': 'question'}, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the number of questions for testing with an equal weight for each source. \n",
    "sample_size = 50\n",
    "print(\"Sampling \", sample_size, \" questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_controversy = df['conversation_type'] == 'controversy guided'\n",
    "controversy_sample = df[is_controversy].sample(2*sample_size//3, random_state=110).reset_index(drop=True)\n",
    "non_controversy_sample = df[~is_controversy].sample(sample_size//3, random_state=110).reset_index(drop=True)\n",
    "df_questions = pd.concat([controversy_sample, non_controversy_sample]).reset_index(drop=True)\n",
    "print(df_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce sample for duplicates\n",
    "this wasn't working well so i manually sorted them :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_openai_response(question, model='o3-mini'):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        response_format={'json_schema'}\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def find_question_clusters(df):\n",
    "    \"\"\"\n",
    "    Use OpenAI to group similar questions together in one API call.\n",
    "    Returns a dictionary where keys are representative questions and values are lists of similar questions.\n",
    "    \"\"\"\n",
    "    questions = df['question'].tolist()\n",
    "    \n",
    "    prompt = f\"\"\"Given this list of questions, first filter out anything that is not a question (e.g. 'hello' or 'about x' or 'i need help with...' or 'i think x' etc). Second, filter out questions that are not subjective (ie keep only ones where there exist multiple perspectives that people with diverse backgrounds might respond to the question with).\n",
    "Now, between the remaining questions, identify clusters of questions that are asking more or less the same thing. For a group of questions that are more or less the same, pick one as the representative. The representative question should be the one that is most neutrally phrased and broad.\n",
    "\n",
    "Return the results as a JSON object where each key is a representative question and its value is a list containing that question and any paraphrases of it. \n",
    "Keep unique subjective questions as-is in the output, even if they don't have paraphrases. DO NOT INCLUDE filtered out questions in the output.\n",
    "\n",
    "Questions:\n",
    "{json.dumps(questions, indent=2)}\n",
    "\n",
    "Example format:\n",
    "{{\n",
    "    \"What is the meaning of life?\": [\n",
    "        \"What is the meaning of life?\",\n",
    "        \"What's life's purpose?\", \n",
    "        \"Why do we exist?\"\n",
    "    ],\n",
    "    \"Should we colonize Mars?\": [\n",
    "        \"Should we colonize Mars?\"\n",
    "    ]\n",
    "}}\"\"\"\n",
    "\n",
    "    response = generate_openai_response(prompt)\n",
    "    \n",
    "    try:\n",
    "        clusters = json.loads(response)\n",
    "    except:\n",
    "        print(\"Failed to parse JSON response from OpenAI\")\n",
    "        clusters = {}\n",
    "        \n",
    "    # Add any questions that weren't included in clusters\n",
    "    processed_questions = set(q for cluster in clusters.values() for q in cluster)\n",
    "    for q in questions:\n",
    "        if q not in processed_questions:\n",
    "            clusters[q] = [q]\n",
    "            \n",
    "    return clusters\n",
    "\n",
    "# Generate clusters of similar questions\n",
    "question_clusters = find_question_clusters(df_questions)\n",
    "\n",
    "print(\"Found\", len(question_clusters), \"unique question clusters:\")\n",
    "for representative_q, similar_qs in question_clusters.items():\n",
    "    \n",
    "    if len(similar_qs) > 1:\n",
    "        print(\"\\nRepresentative question:\", representative_q)\n",
    "        print(\"Similar questions:\")\n",
    "        for q in similar_qs[1:]:\n",
    "            print(\"  -\", q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find questions that weren't clustered\n",
    "all_clustered_questions = set().union(*question_clusters.values())\n",
    "unclustered_questions = list(set(df_questions['question']) - all_clustered_questions)\n",
    "\n",
    "print(f\"\\nFound {len(unclustered_questions)} unclustered questions:\")\n",
    "for q in unclustered_questions:\n",
    "    print(f\"- {q}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['should the legal age of consent for sex be reduced?',\n",
    " 'is gun control a good thing?',\n",
    "#  \"What would it happen if there's no government?\",\n",
    "'Should college be tuition-free?',\n",
    " 'Does religion supersede culture?',\n",
    " 'Do you think men and women were created equally?',\n",
    " 'Is the modern day work schedule (a normal 9 to 6) something we should strive for?',\n",
    " 'What do you think about open relationships?',\n",
    "#  'I used to be a devout Christian but do not really believe in God anymore. Is this normal?',\n",
    " 'Does God exist?',\n",
    "  'Do you think religion has contributed more for peace in society or war?',\n",
    " 'Is America a good country?',\n",
    " 'How would you reorganise politics to reduce wealth inequality?',\n",
    " 'Do you agree with the sentiment that political leaders should meet a higher standard than the average citizen?',\n",
    " 'Is it better to enter a boring relationship just to not be alone, or is it better to remain single?',\n",
    " \"Is it rude to block someone on Facebook due to  them loving Trump and I don't?\",\n",
    " 'I am asked to witness an execution. Should I go?',\n",
    " \"I think university education is worthless for the majority of the population. Most jobs don't require trigonometry or Shakespeare. Companies need workers with practical skills.\",\n",
    "#  'I believe that interbreeding of different races leads to mental instability such as indifference, lack of empathy for others, etc. and violent tendencies',\n",
    " 'What should I do if a rapist was not reported?',\n",
    " \"I cannot decide if it's more important being brave or kind in this world\",\n",
    "]\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def generate_responses(questions, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    \"\"\"\n",
    "    print(\"Generating responses for: \", output_path)\n",
    "    \n",
    "    # Load existing responses if any and if we want to resume\n",
    "    responses = {}\n",
    "    if start_from_checkpoint:\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Get questions that haven't been answered yet for this model\n",
    "    remaining_questions = [\n",
    "        q for q in questions \n",
    "        if q not in responses\n",
    "    ]\n",
    "        \n",
    "    if not remaining_questions:\n",
    "        print(f\"All questions already processed.\")\n",
    "        return\n",
    "                \n",
    "    # Process each remaining question with progress bar\n",
    "    for idx, question in enumerate(tqdm(remaining_questions, desc=f\"Generating responses\", smoothing=0, ascii=True)):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generation_function(question)\n",
    "            \n",
    "            # Store response\n",
    "            responses[question] = response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing question '{question}' for: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            # Save to JSON\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(responses, f, indent=2)\n",
    "                \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running questions through various LLMs\n",
    "\n",
    "\n",
    "### OpenAI Models\n",
    "We're going to start with OpenAI models. You'll need to set your OpenAI API key in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate an LLM response for each question, for each AI model.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_openai_response(question, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# OpenAI models\n",
    "oai_models = ['o3-mini','gpt-4.5-preview']\n",
    "\n",
    "for model in oai_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    generation_function = lambda x: generate_openai_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=questions, \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna run this with Google Deepmind Models. You may need to run:\n",
    "\n",
    "`gcloud components update`\n",
    "\n",
    "`gcloud auth application-default login`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "gdp_models = ['gemma-3-27b-it']\n",
    "for model in gdp_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    generation_function = lambda x: client.models.generate_content(model=model,contents=x).text\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=questions, \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Together AI Models\n",
    "\n",
    "Deeeeeep Seeeeeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, model):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": question}],\n",
    "      max_tokens=2048\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# While you should be able to use the mistral models on HF, together is much faster with a dedicated endpoint and more models.\n",
    "together_models = {\n",
    "    'deepseek-r1': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
    "    'deepseek-v3': 'deepseek-ai/DeepSeek-V3'\n",
    "}\n",
    "\n",
    "for bettername, model in together_models.items():\n",
    "    output_file = bettername+'_responses.json'\n",
    "\n",
    "    generation_function = lambda x: generate_together_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=questions, \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Responses\n",
    "#### We're now going to load in all the responses and make them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in without and sampling or processing\n",
    "# df = pd.read_json(\"hf://datasets/HannahRoseKirk/prism-alignment/conversations.jsonl\", lines=True)\n",
    "# df = df[df['conversation_type']!='unguided']\n",
    "# df.rename(columns={'opening_prompt': 'question'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = oai_models + gdp_models + list(together_models.keys())\n",
    "\n",
    "# Initialize dataframe with questions\n",
    "df = pd.DataFrame({'question': questions})\n",
    "\n",
    "# Add responses for each model\n",
    "for model in all_models:\n",
    "    with open(TEMP_PATH+model+'_responses.json', 'r') as f:\n",
    "        model_responses = json.load(f)\n",
    "        df[model] = df['question'].apply(lambda x: model_responses[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) # Only keep rows where all models have responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to do a basic check to make sure all the model columns are non-null.\n",
    "assert df.isnull().sum().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Create new column with deepseek responses without the thinking part\n",
    "df['deepseek-r1-response'] = df['deepseek-r1'].apply(lambda x: re.sub(r'<think>.*?</think>', '', x, flags=re.DOTALL).strip())\n",
    "\n",
    "# Calculate average length of responses for each column\n",
    "avg_lengths = df.apply(lambda x: x.str.len().mean())\n",
    "print(\"\\nAverage response lengths:\")\n",
    "print(avg_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH+'prism_questions_with_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATA_PATH+'prism_questions_with_responses.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
