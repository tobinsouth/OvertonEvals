{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all synthetic human bios\n",
    "\n",
    "I based my code on the code from [this paper](https://arxiv.org/pdf/2209.06899) (IT WAS PUBLISHED AS A [PDF](https://dataverse.harvard.edu/file.xhtml?fileId=6711665&version=1.0) LIKE WHY) for creating the bio prompts, which takes demographic data from [here](https://faculty.wcas.northwestern.edu/jnd260/pub/Rothschild,%20Howat,%20Shafranek,%20Busby%202018.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniqvals(users, field):\n",
    "    vals = [users[id][field] for id in users.keys()]\n",
    "    return list(set(vals))\n",
    "\n",
    "fields_of_interest = {\n",
    "    \"Gender\": {\n",
    "        \"Male\": \"male\",\n",
    "        \"Female\": \"female\",\n",
    "        '': ''\n",
    "    },\n",
    "    \"Hisp\": {\n",
    "        \"Hispanic\": \"Hispanic\",\n",
    "        \"Not Hispanic\": '',\n",
    "        '': ''\n",
    "    },\n",
    "    \"WHITE\": {\n",
    "        \"White\": \"white\",\n",
    "        \"Non-white\": '',\n",
    "        '': ''\n",
    "    },\n",
    "    \"Ideo\": {\n",
    "        '': '',\n",
    "        'Liberal': 'liberal',\n",
    "        'Slightly conservative': 'slightly conservative',\n",
    "        'Conservative': 'conservative',\n",
    "        'Slightly liberal': 'slightly liberal',\n",
    "        \"Moderate/Haven't thought about it\": 'moderate',\n",
    "        'Extremely Liberal': 'extremely liberal',\n",
    "        'Extremely conservative': 'extremely conservative',\n",
    "    },\n",
    "    \"PID7\": {\n",
    "        '': '',\n",
    "        'Ind': 'am an independent',\n",
    "        'Strong D': 'am a strong Democrat',\n",
    "        'Strong R': 'am a strong Republican',\n",
    "        'Lean D': 'lean towards Democrats',\n",
    "        'Lean R': 'lean towards Rebublicans',\n",
    "        'Weak D': 'am a weak Democrat',\n",
    "        'Weak R': 'am a weak Republican',\n",
    "    },\n",
    "    \"Inc\": {\n",
    "        '': '',\n",
    "        'Less than $15K': 'very poor',\n",
    "        '$15K to $25K': 'poor',\n",
    "        '$25K to $50K': 'poor',\n",
    "        '$50K to $75K': 'middle-class',\n",
    "        '$75K to $100K': 'middle-class',\n",
    "        '$100K to $150K': 'middle-class',\n",
    "        '$150K to $200K': 'upper-class',\n",
    "        '$200K to $250K': 'upper-class',\n",
    "        '$250K to $500K': 'upper-class',\n",
    "        'Prefer not to answer': '',\n",
    "        '-8': '',\n",
    "    },\n",
    "}\n",
    "\n",
    "def mapper(profile):\n",
    "    results = {}\n",
    "    for k in profile.keys():\n",
    "        if k in fields_of_interest:\n",
    "            results[k] = fields_of_interest[k].get(profile[k], '')\n",
    "    if profile['Age'] != '':\n",
    "        age = int(profile['Age'])\n",
    "        if age >= 18 and age < 25:\n",
    "            results['Age'] = 'young'\n",
    "        elif age >= 25 and age < 40: \n",
    "            results['Age'] = 'middle-aged'\n",
    "        elif age >= 40 and age < 60: \n",
    "            results['Age'] = 'old'\n",
    "        elif age >= 60 and age < 100: \n",
    "            results['Age'] = 'very old'\n",
    "        else:\n",
    "            results['Age'] = ''\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../data/ppfull.csv\")\n",
    "\n",
    "# Drop rows where both race categories are empty or have values that will map to empty\n",
    "df = df[\n",
    "    (df['WHITE'].isin(['White'])) |  # Only keep 'White' for WHITE column\n",
    "    (df['Hisp'].isin(['Hispanic']))   # Only keep 'Hispanic' for Hisp column\n",
    "]\n",
    "\n",
    "# Drop rows with empty/NaN values or values that will map to empty for other features\n",
    "df = df[df['Age'].notna() & (df['Age'] != '') & (df['Age'].astype(float) >= 18) & (df['Age'].astype(float) < 100)]\n",
    "df = df[df['Ideo'].isin(fields_of_interest['Ideo'].keys()) & (df['Ideo'] != '')]\n",
    "df = df[df['PID7'].isin(fields_of_interest['PID7'].keys()) & (df['PID7'] != '')]\n",
    "df = df[df['Gender'].isin(fields_of_interest['Gender'].keys()) & (df['Gender'] != '')]\n",
    "df = df[df['Inc'].isin(fields_of_interest['Inc'].keys()) & (df['Inc'] != '')]\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "dmap = df.set_index(df.columns[0]).T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "ids = dmap.keys()\n",
    "for id in tqdm(ids):\n",
    "    user_profile = mapper(dmap[id])\n",
    "    \n",
    "    # Store all features and bio\n",
    "    results[id] = {\n",
    "        'id': id,\n",
    "        'ideology': user_profile.get('Ideo', ''),\n",
    "        'political_affiliation': user_profile.get('PID7', ''),\n",
    "        'race_white': user_profile.get('WHITE', ''),\n",
    "        'hispanic': user_profile.get('Hisp', ''),\n",
    "        'gender': user_profile.get('Gender', ''),\n",
    "        'income': user_profile.get('Inc', ''),\n",
    "        'age': user_profile.get('Age', '')\n",
    "    }\n",
    "    \n",
    "    # Construct bio\n",
    "    prompt = \"\"\n",
    "    if user_profile['Ideo'] != '':\n",
    "        prompt += \"Ideologically, I describe myself as \" + user_profile['Ideo'] + \". \"\n",
    "    if user_profile['PID7'] != '':\n",
    "        prompt += \"Politically, I \" + user_profile['PID7'] + \". \"\n",
    "    if user_profile['WHITE'] == 'White':\n",
    "        prompt += \"Racially, I am white. \"\n",
    "    if user_profile['Hisp'] == 'Hispanic':\n",
    "        prompt += \"Racially, I am Hispanic. \"\n",
    "    if user_profile['Gender'] != '':\n",
    "        prompt += \"I am \" + user_profile['Gender'] + \". \"\n",
    "    if user_profile['Inc'] != '':\n",
    "        prompt += \"Financially, I am \" + user_profile['Inc'] + \". \"\n",
    "    if user_profile.get('Age', '') != '':\n",
    "        prompt += \"In terms of my age, I am \" + user_profile['Age'] + \". \"\n",
    "    \n",
    "    results[id]['bio'] = prompt\n",
    "\n",
    "# Convert to DataFrame (you can add this after the loop)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres a lot of duplicates, dont need 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each duplicated row, excluding the (unique) id\n",
    "duplicate_value_counts = df.drop(columns=['id']).value_counts()\n",
    "\n",
    "# TODO: sample weighted by these counts\n",
    "\n",
    "# Display the value counts of duplicated rows\n",
    "duplicate_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with unique rows and their counts\n",
    "unique_with_counts_df = df.drop(columns=['id']).value_counts().reset_index(name='count')\n",
    "\n",
    "# Save the DataFrame to a CSV file with counts for future sampling \n",
    "unique_with_counts_df.to_csv('../data/pigeonhole_human_data_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df with only the unique rows\n",
    "unique_df = df.drop_duplicates(subset=df.columns.difference(['id']))\n",
    "\n",
    "# Save the unique DataFrame to a CSV file\n",
    "unique_df.to_csv('../data/pigeonhole_human_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating their responses on some Habermas questions using some models\n",
    "for now: \n",
    "1. sample subset of 50 users\n",
    "2. sample 5 Habermas questions \n",
    "3. generate for llama-3.1-8b-instruct & gpt-4o-mini\n",
    "\n",
    "this bit of code will look a lot like `generating_llm_responses.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling 50 \"humans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_human = unique_with_counts_df.sample(5, weights='count', random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions + LLM responses data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv')\n",
    "if 'Unnamed: 0' in df_questions.columns:\n",
    "    df_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(\"df_questions.shape: \", df_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_qs = df_questions.sample(5,random_state=42).reset_index(drop=True)\n",
    "sample_qs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Human responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_responses(questions, bios, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    \"\"\"\n",
    "    print(\"Generating responses for: \", output_path)\n",
    "    \n",
    "    # Load existing responses if any and if we want to resume\n",
    "    responses = {}\n",
    "    if start_from_checkpoint:\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Get questions that haven't been answered yet for this model\n",
    "    remaining_questions = [\n",
    "        q for q in questions \n",
    "        if q not in responses\n",
    "    ]\n",
    "        \n",
    "    if not remaining_questions:\n",
    "        print(f\"All questions already processed.\")\n",
    "        return\n",
    "             \n",
    "    for bio in tqdm(bios,desc='Generating for bio'):\n",
    "        # Process each remaining question with progress bar\n",
    "        for idx, question in enumerate(tqdm(remaining_questions, desc=f\"Generating question responses\", smoothing=0, ascii=True)):\n",
    "            try:\n",
    "                # Generate response\n",
    "                response = generation_function(question, bio)\n",
    "                \n",
    "                # Store response\n",
    "                if question not in responses:\n",
    "                    responses[question] = {}\n",
    "                responses[question][bio] = response\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing question '{question}' for: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                # Save to JSON\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(responses, f, indent=2)\n",
    "                \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, bio, model):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": f\"Answer only from the perspective of a person with the following demographics and beliefs:\\n{bio}\"},\n",
    "          {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "      max_tokens=2048\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# While you should be able to use the mistral models on HF, together is much faster with a dedicated endpoint and more models.\n",
    "together_models = {\n",
    "    # 'mistral-7b-instruct': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    'llama-3.1-8b-instruct': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K',\n",
    "    # 'deepseek-r1': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B',\n",
    "    # 'gemma-2b-it': 'google/gemma-2b-it'\n",
    "}\n",
    "for bettername, model in together_models.items():\n",
    "    output_file = bettername+'_responses.json'\n",
    "\n",
    "    generation_function = lambda x, bio: generate_together_response(x, bio, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=sample_qs['question'], \n",
    "        bios=sample_human['bio'].values.tolist(),\n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_to_df(questions_df, bios_df):\n",
    "    \"\"\"\n",
    "    Generate responses and format them into a DataFrame with demographic information\n",
    "    \"\"\"\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K'\n",
    "    all_results = []\n",
    "    \n",
    "    # Loop through each bio\n",
    "    for _, bio_row in tqdm(bios_df.iterrows(), desc='Generating responses'):\n",
    "        # For each bio, generate response for each question\n",
    "        for _, question_row in questions_df.iterrows():\n",
    "            try:\n",
    "                response = generate_together_response(question_row['question'], bio_row['bio'], model)\n",
    "                \n",
    "                # Combine question, response, and demographic info\n",
    "                result = {\n",
    "                    'question': question_row['question'],\n",
    "                    'response': response,\n",
    "                    # Add all demographic columns\n",
    "                    'ideology': bio_row['ideology'],\n",
    "                    'political_affiliation': bio_row['political_affiliation'],\n",
    "                    'race_white': bio_row['race_white'],\n",
    "                    'hispanic': bio_row['hispanic'],\n",
    "                    'gender': bio_row['gender'],\n",
    "                    'income': bio_row['income'],\n",
    "                    'age': bio_row['age'],\n",
    "                    'bio': bio_row['bio']\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing question for bio: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "# Generate responses\n",
    "final_df = generate_responses_to_df(\n",
    "    questions_df=sample_qs,\n",
    "    bios_df=sample_human\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(TEMP_PATH + 'llama_responses_with_demographics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class AgreementRating(BaseModel):\n",
    "    explanation: str = Field(description=\"Brief rationale for the chosen rating\")\n",
    "    answer: Literal[1, 2, 3, 4, 5] = Field(description=\"Agreement rating on a 1-5 scale\")\n",
    "    quotes: list[str] = Field(description=\"List of substrings from the response that the user feels specifically represented by\")\n",
    "\n",
    "def generate_agreement_rating(question, llm_response, bio, human_answer, model):\n",
    "    \"\"\"\n",
    "    Generate structured agreement rating for an LLM response from a human perspective\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Answer only from the perspective of a person with the following demographics and beliefs:\n",
    "{bio}\n",
    "\n",
    "In response to this question: '{question},' you said '{human_answer}'\"\"\"\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Response: {llm_response}\n",
    "\n",
    "On a scale of 1-5, to what extent does the above response represent *your* opinion?\n",
    "1 = Completely disagree\n",
    "2 = Somewhat disagree\n",
    "3 = Neutral\n",
    "4 = Somewhat agree\n",
    "5 = Completely agree\n",
    "\n",
    "Provide your rating, a brief explanation for your choice, and a list of exact quotes from the response that you feel represent your opinion, if any.\n",
    "\n",
    "Output should be in the following JSON format:\n",
    "{{\n",
    "    \"explanation\": \"Brief rationale for your rating\",\n",
    "    \"answer\": <integer between 1-5>,\n",
    "    \"quotes\": [\"quote1\", \"quote2\", ...]\n",
    "}}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=2048,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response_dict = json.loads(completion.choices[0].message.content)\n",
    "        rating = AgreementRating(**response_dict)\n",
    "        return rating\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_agreement_ratings_df(questions_df, bios_df, llm_to_eval, human_responses_df):\n",
    "    \"\"\"\n",
    "    Generate agreement ratings for each human-LLM response pair\n",
    "    \"\"\"\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n",
    "    all_results = []\n",
    "    \n",
    "    for _, bio_row in tqdm(bios_df.iterrows(), desc='Processing bios'):\n",
    "        for _, question_row in questions_df.iterrows():\n",
    "            # Get LLM response for this question\n",
    "            llm_response = question_row[llm_to_eval]\n",
    "            \n",
    "            # Get human response for this question and bio\n",
    "            human_response = human_responses_df[\n",
    "                (human_responses_df['question'] == question_row['question']) &\n",
    "                (human_responses_df['bio'] == bio_row['bio'])\n",
    "            ]['response'].iloc[0]\n",
    "            \n",
    "            try:\n",
    "                rating = generate_agreement_rating(\n",
    "                    question_row['question'],\n",
    "                    llm_response,\n",
    "                    bio_row['bio'],\n",
    "                    human_response,\n",
    "                    model\n",
    "                )\n",
    "                \n",
    "                if rating:\n",
    "                    result = {\n",
    "                        'question': question_row['question'],\n",
    "                        'llm': llm_to_eval,\n",
    "                        'llm_response': llm_response,\n",
    "                        'human_response': human_response,\n",
    "                        'agreement_rating': rating.answer,\n",
    "                        'rating_explanation': rating.explanation,\n",
    "                        'quotes': rating.quotes,\n",
    "                        'ideology': bio_row['ideology'],\n",
    "                        'political_affiliation': bio_row['political_affiliation'],\n",
    "                        'race_white': bio_row['race_white'],\n",
    "                        'hispanic': bio_row['hispanic'],\n",
    "                        'gender': bio_row['gender'],\n",
    "                        'income': bio_row['income'],\n",
    "                        'age': bio_row['age'],\n",
    "                        'bio': bio_row['bio'],\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing rating: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "# Generate agreement ratings\n",
    "agreement_df = generate_agreement_ratings_df(\n",
    "    questions_df=sample_qs,\n",
    "    bios_df=sample_human,\n",
    "    llm_to_eval='llama-3.1-8b-instruct',\n",
    "    human_responses_df=final_df\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "agreement_df.to_csv(TEMP_PATH + 'llama_agreement_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
