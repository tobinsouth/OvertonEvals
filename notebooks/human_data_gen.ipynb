{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all synthetic human bios\n",
    "\n",
    "I based my code on the code from [this paper](https://arxiv.org/pdf/2209.06899) (IT WAS PUBLISHED AS A [PDF](https://dataverse.harvard.edu/file.xhtml?fileId=6711665&version=1.0) LIKE WHY) for creating the bio prompts, which takes demographic data from [here](https://faculty.wcas.northwestern.edu/jnd260/pub/Rothschild,%20Howat,%20Shafranek,%20Busby%202018.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uniqvals(users, field):\n",
    "    vals = [users[id][field] for id in users.keys()]\n",
    "    return list(set(vals))\n",
    "\n",
    "fields_of_interest = {\n",
    "    \"Gender\": {\n",
    "        \"Male\": \"male\",\n",
    "        \"Female\": \"female\",\n",
    "        '': ''\n",
    "    },\n",
    "    \"Hisp\": {\n",
    "        \"Hispanic\": \"Hispanic\",\n",
    "        \"Not Hispanic\": '',\n",
    "        '': ''\n",
    "    },\n",
    "    \"WHITE\": {\n",
    "        \"White\": \"white\",\n",
    "        \"Non-white\": '',\n",
    "        '': ''\n",
    "    },\n",
    "    \"Ideo\": {\n",
    "        '': '',\n",
    "        'Liberal': 'liberal',\n",
    "        'Slightly conservative': 'slightly conservative',\n",
    "        'Conservative': 'conservative',\n",
    "        'Slightly liberal': 'slightly liberal',\n",
    "        \"Moderate/Haven't thought about it\": 'moderate',\n",
    "        'Extremely Liberal': 'extremely liberal',\n",
    "        'Extremely conservative': 'extremely conservative',\n",
    "    },\n",
    "    \"PID7\": {\n",
    "        '': '',\n",
    "        'Ind': 'am an independent',\n",
    "        'Strong D': 'am a strong Democrat',\n",
    "        'Strong R': 'am a strong Republican',\n",
    "        'Lean D': 'lean towards Democrats',\n",
    "        'Lean R': 'lean towards Rebublicans',\n",
    "        'Weak D': 'am a weak Democrat',\n",
    "        'Weak R': 'am a weak Republican',\n",
    "    },\n",
    "    \"Inc\": {\n",
    "        '': '',\n",
    "        'Less than $15K': 'very poor',\n",
    "        '$15K to $25K': 'poor',\n",
    "        '$25K to $50K': 'poor',\n",
    "        '$50K to $75K': 'middle-class',\n",
    "        '$75K to $100K': 'middle-class',\n",
    "        '$100K to $150K': 'middle-class',\n",
    "        '$150K to $200K': 'upper-class',\n",
    "        '$200K to $250K': 'upper-class',\n",
    "        '$250K to $500K': 'upper-class',\n",
    "        'Prefer not to answer': '',\n",
    "        '-8': '',\n",
    "    },\n",
    "}\n",
    "\n",
    "def mapper(profile):\n",
    "    results = {}\n",
    "    for k in profile.keys():\n",
    "        if k in fields_of_interest:\n",
    "            results[k] = fields_of_interest[k].get(profile[k], '')\n",
    "    if profile['Age'] != '':\n",
    "        age = int(profile['Age'])\n",
    "        if age >= 18 and age < 25:\n",
    "            results['Age'] = 'young'\n",
    "        elif age >= 25 and age < 40: \n",
    "            results['Age'] = 'middle-aged'\n",
    "        elif age >= 40 and age < 60: \n",
    "            results['Age'] = 'old'\n",
    "        elif age >= 60 and age < 100: \n",
    "            results['Age'] = 'very old'\n",
    "        else:\n",
    "            results['Age'] = ''\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../data/ppfull.csv\")\n",
    "\n",
    "# Drop rows where both race categories are empty or have values that will map to empty\n",
    "df = df[\n",
    "    (df['WHITE'].isin(['White'])) |  # Only keep 'White' for WHITE column\n",
    "    (df['Hisp'].isin(['Hispanic']))   # Only keep 'Hispanic' for Hisp column\n",
    "]\n",
    "\n",
    "# Drop rows with empty/NaN values or values that will map to empty for other features\n",
    "df = df[df['Age'].notna() & (df['Age'] != '') & (df['Age'].astype(float) >= 18) & (df['Age'].astype(float) < 100)]\n",
    "df = df[df['Ideo'].isin(fields_of_interest['Ideo'].keys()) & (df['Ideo'] != '')]\n",
    "df = df[df['PID7'].isin(fields_of_interest['PID7'].keys()) & (df['PID7'] != '')]\n",
    "df = df[df['Gender'].isin(fields_of_interest['Gender'].keys()) & (df['Gender'] != '')]\n",
    "df = df[df['Inc'].isin(fields_of_interest['Inc'].keys()) & (df['Inc'] != '')]\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "dmap = df.set_index(df.columns[0]).T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "ids = dmap.keys()\n",
    "for id in tqdm(ids):\n",
    "    user_profile = mapper(dmap[id])\n",
    "    \n",
    "    # Store all features and bio\n",
    "    results[id] = {\n",
    "        'id': id,\n",
    "        'ideology': user_profile.get('Ideo', ''),\n",
    "        'political_affiliation': user_profile.get('PID7', ''),\n",
    "        'race_white': user_profile.get('WHITE', ''),\n",
    "        'hispanic': user_profile.get('Hisp', ''),\n",
    "        'gender': user_profile.get('Gender', ''),\n",
    "        'income': user_profile.get('Inc', ''),\n",
    "        'age': user_profile.get('Age', '')\n",
    "    }\n",
    "    \n",
    "    # Construct bio\n",
    "    prompt = \"\"\n",
    "    if user_profile['Ideo'] != '':\n",
    "        prompt += \"Ideologically, I describe myself as \" + user_profile['Ideo'] + \". \"\n",
    "    if user_profile['PID7'] != '':\n",
    "        prompt += \"Politically, I \" + user_profile['PID7'] + \". \"\n",
    "    if user_profile['WHITE'] == 'White':\n",
    "        prompt += \"Racially, I am white. \"\n",
    "    if user_profile['Hisp'] == 'Hispanic':\n",
    "        prompt += \"Racially, I am Hispanic. \"\n",
    "    if user_profile['Gender'] != '':\n",
    "        prompt += \"I am \" + user_profile['Gender'] + \". \"\n",
    "    if user_profile['Inc'] != '':\n",
    "        prompt += \"Financially, I am \" + user_profile['Inc'] + \". \"\n",
    "    if user_profile.get('Age', '') != '':\n",
    "        prompt += \"In terms of my age, I am \" + user_profile['Age'] + \". \"\n",
    "    \n",
    "    results[id]['bio'] = prompt\n",
    "\n",
    "# Convert to DataFrame (you can add this after the loop)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theres a lot of duplicates, dont need 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each duplicated row, excluding the (unique) id\n",
    "duplicate_value_counts = df.drop(columns=['id']).value_counts()\n",
    "\n",
    "# TODO: sample weighted by these counts\n",
    "\n",
    "# Display the value counts of duplicated rows\n",
    "duplicate_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with unique rows and their counts\n",
    "unique_with_counts_df = df.drop(columns=['id']).value_counts().reset_index(name='count')\n",
    "\n",
    "# Save the DataFrame to a CSV file with counts for future sampling \n",
    "unique_with_counts_df.to_csv('../data/pigeonhole_human_data_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df with only the unique rows\n",
    "unique_df = df.drop_duplicates(subset=df.columns.difference(['id']))\n",
    "\n",
    "# Save the unique DataFrame to a CSV file\n",
    "unique_df.to_csv('../data/pigeonhole_human_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating their responses on some Habermas questions using some models\n",
    "for now: \n",
    "1. sample subset of 50 users\n",
    "2. sample 5 Habermas questions \n",
    "3. generate for llama-3.1-8b-instruct & gpt-4o-mini\n",
    "\n",
    "this bit of code will look a lot like `generating_llm_responses.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling N \"humans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_human = 15\n",
    "sample_human = unique_with_counts_df.sample(N_human, weights='count', random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions + LLM responses data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv')\n",
    "if 'Unnamed: 0' in df_questions.columns:\n",
    "    df_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(\"df_questions.shape: \", df_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_qs = 5\n",
    "sample_qs = df_questions.sample(N_qs,random_state=42).reset_index(drop=True)\n",
    "sample_qs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Human responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this works but it only saves as a json with questions as keys, then values are dict with key of bios and values as responses. \n",
    "# # commenting out so i can look back for posterity but the next cells contain the actual pipeline code\n",
    "\n",
    "def generate_responses(questions, bios, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    # This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    Generate responses from an LLM for each bio-question pair, with checkpointing.\n",
    "    \"\"\"\n",
    "    print(\"Generating responses for:\", output_path)\n",
    "    \n",
    "    # Load existing responses if any\n",
    "    responses = {}\n",
    "    if start_from_checkpoint and os.path.exists(output_path):\n",
    "        print('Loaded checkpoint file!')\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Track which question-bio pairs need to be generated\n",
    "    for question in tqdm(questions, desc='Processing questions'):\n",
    "        if question not in responses:\n",
    "            responses[question] = {}\n",
    "            \n",
    "        # Get bios that haven't been answered yet for this question\n",
    "        remaining_bios = [\n",
    "            b for b in bios \n",
    "            if b not in responses[question]\n",
    "        ]\n",
    "            \n",
    "        if not remaining_bios:\n",
    "            print(f\"All bios already processed for question.\")\n",
    "            continue\n",
    "        else: \n",
    "            print(len(bios)-len(remaining_bios), \"skipped! Beginning the remaining\", len(remaining_bios), \"now...\")\n",
    "                \n",
    "        for bio in tqdm(remaining_bios, desc=\"Generating responses for bios\"):\n",
    "            try:\n",
    "                response = generation_function(question, bio)\n",
    "                responses[question][bio] = response\n",
    "                \n",
    "                # Checkpoint save every 10 bios\n",
    "                if len(responses[question]) % 10 == 0:\n",
    "                    with open(output_path, 'w') as f:\n",
    "                        json.dump(responses, f, indent=2)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing bio for question '{question}': {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Final save\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, bio, model):\n",
    "    \"\"\"\n",
    "    Generate a response using the Together API for a given question and bio.\n",
    "\n",
    "    This function sends a request to the Together API to generate a response\n",
    "    based on the provided question and bio. The response is generated using\n",
    "    a specified model, which is passed as an argument to the function.\n",
    "\n",
    "    Parameters:\n",
    "    - question (str): The question to be answered.\n",
    "    - bio (str): The demographic and belief information of the person whose perspective\n",
    "      should be considered when generating the response.\n",
    "    - model (str): The model identifier to be used for generating the response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated response content from the Together API.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": f\"Answer only from the perspective of a person with the following demographics and beliefs:\\n{bio}\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses_to_df(questions_df, bios_df, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    Generate responses and format them into a DataFrame with demographic information,\n",
    "    with checkpointing functionality.\n",
    "    \"\"\"\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K'\n",
    "    all_results = []\n",
    "    \n",
    "    # Load existing results if checkpoint exists\n",
    "    if start_from_checkpoint and os.path.exists(output_path):\n",
    "        print('Loading from checkpoint file...')\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        all_results = existing_df.to_dict('records')\n",
    "        \n",
    "        # Create set of existing question-bio pairs\n",
    "        existing_pairs = set(\n",
    "            (row['question'], row['bio']) \n",
    "            for row in all_results\n",
    "        )\n",
    "    else:\n",
    "        existing_pairs = set()\n",
    "    \n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Loop through each bio\n",
    "    for _, bio_row in tqdm(bios_df.iterrows(), desc='Processing bios', total=len(bios_df), position=0):\n",
    "        for _, question_row in tqdm(questions_df.iterrows(), desc=\"Processing questions for bio\", total=len(questions_df), leave=False, position=1):\n",
    "            # Skip if this pair was already processed\n",
    "            if (question_row['question'], bio_row['bio']) in existing_pairs:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                response = generate_together_response(question_row['question'], bio_row['bio'], model)\n",
    "                \n",
    "                # Combine question, response, and demographic info\n",
    "                result = {\n",
    "                    'question': question_row['question'],\n",
    "                    'response': response,\n",
    "                    'ideology': bio_row['ideology'],\n",
    "                    'political_affiliation': bio_row['political_affiliation'],\n",
    "                    'race_white': bio_row['race_white'],\n",
    "                    'hispanic': bio_row['hispanic'],\n",
    "                    'gender': bio_row['gender'],\n",
    "                    'income': bio_row['income'],\n",
    "                    'age': bio_row['age'],\n",
    "                    'bio': bio_row['bio']\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Checkpoint save every 10 responses\n",
    "                if len(all_results) % 10 == 0:\n",
    "                    pd.DataFrame(all_results).to_csv(output_path, index=False)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing question '{question_row['question']}' for bio: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Final save\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    return results_df\n",
    "\n",
    "# Generate responses with checkpointing\n",
    "final_df = generate_responses_to_df(\n",
    "    questions_df=sample_qs,\n",
    "    bios_df=sample_human,\n",
    "    output_path=TEMP_PATH + 'llama_responses_with_demographics.csv',\n",
    "    start_from_checkpoint=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(TEMP_PATH + 'llama_responses_with_demographics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class AgreementRating(BaseModel):\n",
    "    explanation: str = Field(description=\"Brief rationale for the chosen rating\")\n",
    "    answer: Literal[1, 2, 3, 4, 5] = Field(description=\"Agreement rating on a 1-5 scale\")\n",
    "    quotes: list[str] = Field(description=\"List of substrings from the response that the user feels specifically represented by\")\n",
    "\n",
    "def generate_agreement_rating(question, llm_response, bio, human_answer, model):\n",
    "    \"\"\"\n",
    "    Generate structured agreement rating for an LLM response from a human perspective\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Answer only from the perspective of a person with the following demographics and beliefs:\n",
    "{bio}\n",
    "\n",
    "In response to this question: '{question},' your opinion is: '{human_answer}'\"\"\"\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Response: {llm_response}\n",
    "\n",
    "On a scale of 1-5, to what extent does the above response represent *your* opinion?\n",
    "1 = Completely disagree\n",
    "2 = Somewhat disagree\n",
    "3 = Neutral\n",
    "4 = Somewhat agree\n",
    "5 = Completely agree\n",
    "\n",
    "Provide your rating, a brief explanation for your rating, and a list of exact quotes from the response that you feel represent your opinion, if any.\n",
    "\n",
    "Output should be in the following JSON format:\n",
    "{{\n",
    "    \"explanation\": \"Brief rationale for your rating\",\n",
    "    \"answer\": <integer between 1-5>,\n",
    "    \"quotes\": [\"quote1\", \"quote2\", ...]\n",
    "}}\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=2048,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response_dict = json.loads(completion.choices[0].message.content)\n",
    "        rating = AgreementRating(**response_dict)\n",
    "        return rating\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to find the best substring match for inexact quotes (usually formatting stuff) \n",
    "# and it returns the % of matched characters so i can set a threshold for precision\n",
    "# right now its set to 85%\n",
    "def find_best_substring_match(text: str, quote: str) -> tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Find the longest matching substring and its match ratio\n",
    "    \n",
    "    Args:\n",
    "        text: The full text to search in\n",
    "        quote: The quote to match\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best matching substring, ratio of match length to quote length)\n",
    "    \"\"\"\n",
    "    words = quote.split()\n",
    "    best_match = \"\"\n",
    "    best_ratio = 0\n",
    "    \n",
    "    # Try all possible contiguous subsequences of words\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words) + 1):\n",
    "            substring = \" \".join(words[i:j])\n",
    "            if substring in text:\n",
    "                ratio = len(substring) / len(quote)\n",
    "                if ratio > best_ratio:\n",
    "                    best_ratio = ratio\n",
    "                    best_match = substring\n",
    "    \n",
    "    return best_match, best_ratio\n",
    "\n",
    "def validate_quotes(llm_response: str, quotes: list[str], match_threshold: float = 0.85, verbose: bool = False) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Validate quotes against response, allowing for partial matches above threshold\n",
    "    \n",
    "    Args:\n",
    "        llm_response: The full response from the LLM\n",
    "        quotes: List of quoted strings that should appear in the response\n",
    "        match_threshold: Minimum ratio of match length to quote length (default: 0.9)\n",
    "        verbose: If True, print detailed validation process (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (validation success, list of validated/corrected quotes)\n",
    "    \"\"\"\n",
    "    all_valid = True\n",
    "    validated_quotes = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nValidating quotes in response:\")\n",
    "        print(f\"Full response: {llm_response}\\n\")\n",
    "    \n",
    "    for quote in quotes:\n",
    "        if quote in llm_response:\n",
    "            if verbose:\n",
    "                print(f\"✓ Found exact quote: {quote}\")\n",
    "            validated_quotes.append(quote)\n",
    "        else:\n",
    "            best_match, match_ratio = find_best_substring_match(llm_response, quote)\n",
    "            \n",
    "            if match_ratio >= match_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"~ Found close match for: {quote}\")\n",
    "                    print(f\"  Best match ({match_ratio:.1%}): {best_match}\")\n",
    "                validated_quotes.append(best_match)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"✗ Missing quote: {quote}\")\n",
    "                    if best_match:\n",
    "                        print(f\"  Best partial match ({match_ratio:.1%}): {best_match}\")\n",
    "                all_valid = False\n",
    "    \n",
    "    if verbose:\n",
    "        if not all_valid:\n",
    "            print(\"\\nQuote validation failed!\")\n",
    "        else:\n",
    "            print(\"\\nAll quotes validated successfully!\")\n",
    "        \n",
    "    return all_valid, validated_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_agreement_rating_with_retry(question, llm_response, bio, human_answer, model, retry=1):\n",
    "    \"\"\"\n",
    "    Generate agreement rating with quote validation and retry logic\n",
    "    \n",
    "    Args:\n",
    "        question: The question being rated\n",
    "        llm_response: The LLM's response to rate\n",
    "        bio: The demographic/belief profile\n",
    "        human_answer: The human's answer\n",
    "        model: The model to use for generation\n",
    "        retry: Number of retries if quote validation fails (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        AgreementRating or None: Valid rating object or None if all attempts fail\n",
    "    \"\"\"\n",
    "    attempts = 0\n",
    "    while attempts <= retry:\n",
    "        try:\n",
    "            rating = generate_agreement_rating(question, llm_response, bio, human_answer, model)\n",
    "            \n",
    "            if rating:\n",
    "                is_valid, validated_quotes = validate_quotes(llm_response, rating.quotes)\n",
    "                if is_valid:\n",
    "                    # Update rating with validated quotes if any were modified\n",
    "                    rating.quotes = validated_quotes\n",
    "                    return rating\n",
    "            \n",
    "            attempts += 1\n",
    "            if attempts <= retry:\n",
    "                print(f\"\\nQuote validation failed, attempt {attempts}/{retry}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in generation attempt {attempts}: {str(e)}\")\n",
    "            attempts += 1\n",
    "            \n",
    "    print(\"\\nAll attempts failed to generate valid quotes\")\n",
    "    return None\n",
    "\n",
    "# Update the main function to use the retry logic\n",
    "def generate_agreement_ratings_df(questions_df, bios_df, llm_to_eval, human_responses_df, checkpoint_path=None, retry=1):\n",
    "    \"\"\"\n",
    "    Generate agreement ratings for each human-LLM response pair, with checkpoint functionality\n",
    "    \n",
    "    Args:\n",
    "        questions_df: DataFrame containing questions and LLM responses\n",
    "        bios_df: DataFrame containing demographic information and bios\n",
    "        llm_to_eval: Name of LLM column to evaluate\n",
    "        human_responses_df: DataFrame containing human responses\n",
    "        checkpoint_path: Optional path to checkpoint file\n",
    "        retry: Number of retries for quote validation (default: 1)\n",
    "    \"\"\"\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'\n",
    "    all_results = []\n",
    "    \n",
    "    # Load existing results if checkpoint exists\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print('Loading from checkpoint file...')\n",
    "        existing_df = pd.read_csv(checkpoint_path)\n",
    "        all_results = existing_df.to_dict('records')\n",
    "        \n",
    "        # Create set of existing question-bio pairs\n",
    "        existing_pairs = set(\n",
    "            (row['question'], row['bio'], row['llm']) \n",
    "            for row in all_results\n",
    "        )\n",
    "    else:\n",
    "        existing_pairs = set()\n",
    "    \n",
    "    # Make sure checkpoint directory exists if path provided\n",
    "    if checkpoint_path:\n",
    "        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    for bio_idx, bio_row in tqdm(bios_df.iterrows(), \n",
    "                                desc='Processing bios', \n",
    "                                total=len(bios_df),\n",
    "                                position=0):\n",
    "        for q_idx, question_row in tqdm(questions_df.iterrows(), \n",
    "                                      desc='Processing questions', \n",
    "                                      total=len(questions_df),\n",
    "                                      leave=False, \n",
    "                                      position=1):\n",
    "            # Skip if this combination already exists in checkpoint\n",
    "            if (question_row['question'], bio_row['bio'], llm_to_eval) in existing_pairs:\n",
    "                continue\n",
    "                \n",
    "            llm_response = question_row[llm_to_eval]\n",
    "            human_response = human_responses_df[\n",
    "                (human_responses_df['question'] == question_row['question']) &\n",
    "                (human_responses_df['bio'] == bio_row['bio'])\n",
    "            ]['response'].iloc[0]\n",
    "            \n",
    "            try:\n",
    "                rating = generate_agreement_rating_with_retry(\n",
    "                    question_row['question'],\n",
    "                    llm_response,\n",
    "                    bio_row['bio'],\n",
    "                    human_response,\n",
    "                    model,\n",
    "                    retry=retry\n",
    "                )\n",
    "                \n",
    "                if rating: # only save to results if the above doesn't fail\n",
    "                    result = {\n",
    "                        'question': question_row['question'],\n",
    "                        'llm': llm_to_eval,\n",
    "                        'llm_response': llm_response,\n",
    "                        'human_response': human_response,\n",
    "                        'agreement_rating': rating.answer,\n",
    "                        'rating_explanation': rating.explanation,\n",
    "                        'quotes': rating.quotes,\n",
    "                        'ideology': bio_row['ideology'],\n",
    "                        'political_affiliation': bio_row['political_affiliation'],\n",
    "                        'race_white': bio_row['race_white'],\n",
    "                        'hispanic': bio_row['hispanic'],\n",
    "                        'gender': bio_row['gender'],\n",
    "                        'income': bio_row['income'],\n",
    "                        'age': bio_row['age'],\n",
    "                        'bio': bio_row['bio'],\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    # Save checkpoint every 10 new results\n",
    "                    if checkpoint_path and len(all_results) % 10 == 0:\n",
    "                        pd.DataFrame(all_results).to_csv(checkpoint_path, index=False)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing rating: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Final save if checkpoint path provided\n",
    "    if checkpoint_path:\n",
    "        results_df.to_csv(checkpoint_path, index=False)\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "checkpoint_path = TEMP_PATH + 'llama_agreement_ratings.csv'\n",
    "agreement_df = generate_agreement_ratings_df(\n",
    "    questions_df=sample_qs,\n",
    "    bios_df=sample_human,\n",
    "    llm_to_eval='llama-3.1-8b-instruct',\n",
    "    human_responses_df=final_df,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    retry=2  # Will try up to 3 times total (initial + 2 retries)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if checkpoint is disabled above and you want to save the results anyway \n",
    "# save_path = TEMP_PATH + 'llama_agreement_ratings.csv'\n",
    "# agreement_df.to_csv(save_path)\n",
    "agreement_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots! Statistics!\n",
    "\n",
    "lets calculate average agreement by the various demographic categories and plot those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average agreement rating by llm and demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def setup_plot_style(fig: plt.Figure, ax: plt.Axes) -> None:\n",
    "    \"\"\"Set up the basic plot style and appearance.\"\"\"\n",
    "    # Set white background\n",
    "    fig.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # Remove unnecessary spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "def calculate_statistics(df: pd.DataFrame, llm: str) -> pd.DataFrame:\n",
    "    \"\"\"Calculate mean and std of agreement ratings for a given LLM.\"\"\"\n",
    "    llm_data = df[df['llm'] == llm]\n",
    "    return llm_data.groupby('question')['agreement_rating'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "def create_grouped_bar_plot(agreement_df: pd.DataFrame, \n",
    "                          bar_width: float = 0.25,\n",
    "                          figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing agreement ratings across questions and LLMs.\n",
    "    \n",
    "    Args:\n",
    "        agreement_df: DataFrame containing agreement ratings\n",
    "        bar_width: Width of each bar in the plot\n",
    "        figsize: Size of the figure (width, height)\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    llms = agreement_df['llm'].unique()\n",
    "    questions = agreement_df['question'].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Plot bars for each LLM\n",
    "    for idx, llm in enumerate(llms):\n",
    "        stats = calculate_statistics(agreement_df, llm)\n",
    "        position = r + bar_width * idx\n",
    "        ax.bar(position, stats['mean'], bar_width, \n",
    "               yerr=stats['std'], label=llm, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r + bar_width)\n",
    "    ax.set_xticklabels(short_questions, rotation=45, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Average Agreement Rating')\n",
    "    ax.set_title('Average Agreement Ratings by Question and LLM')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Final adjustments\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Create the plot\n",
    "create_grouped_bar_plot(agreement_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographic_bar_plot(agreement_df: pd.DataFrame,\n",
    "                              llm: str,\n",
    "                              demographic_col: str,\n",
    "                              figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing agreement ratings across questions,\n",
    "    grouped by a demographic category for a single LLM.\n",
    "    \n",
    "    Args:\n",
    "        agreement_df: DataFrame containing agreement ratings\n",
    "        llm: Name of the LLM to analyze\n",
    "        demographic_col: Name of demographic column to group by\n",
    "        figsize: Size of the figure (width, height)\n",
    "    \"\"\"\n",
    "    # Filter for specific LLM\n",
    "    llm_data = agreement_df[agreement_df['llm'] == llm]\n",
    "    \n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    questions = llm_data['question'].unique()\n",
    "    demographic_values = llm_data[demographic_col].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Calculate appropriate bar width\n",
    "    # Total width available per group = 0.8 (leaving 0.2 for spacing between groups)\n",
    "    total_width_per_group = 0.8\n",
    "    bar_width = total_width_per_group / len(demographic_values)\n",
    "    \n",
    "    # Calculate offset to center the group of bars\n",
    "    group_center_offset = (total_width_per_group - bar_width) / 2\n",
    "    \n",
    "    # Plot bars for each demographic value\n",
    "    for idx, demo_value in enumerate(demographic_values):\n",
    "        # Calculate statistics for this demographic group\n",
    "        demo_data = llm_data[llm_data[demographic_col] == demo_value]\n",
    "        stats = demo_data.groupby('question')['agreement_rating'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        # Plot bars\n",
    "        # Position each bar relative to the center of the group\n",
    "        position = r + (idx * bar_width) - group_center_offset\n",
    "        ax.bar(position, stats['mean'], bar_width,\n",
    "               yerr=stats['std'], label=demo_value, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r)  # Set ticks at the center of each group\n",
    "    ax.set_xticklabels(short_questions, rotation=15, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Average Agreement Rating')\n",
    "    ax.set_title(f'Average Agreement Ratings by Question and {demographic_col}\\nfor {llm}')\n",
    "    \n",
    "    \n",
    "    # Set y-axis limits to show full range of ratings (1-5)\n",
    "    ax.set_ylim(0.5, 5.5)  # Slightly expanded range for visibility\n",
    "    \n",
    "    # Adjust subplot parameters to make room for legend at bottom\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Place legend horizontally at bottom\n",
    "    ax.legend(title=demographic_col, \n",
    "             bbox_to_anchor=(0.5, -0.2),  # Position below plot\n",
    "             loc='upper center',           # Center horizontally\n",
    "             ncol=len(demographic_values), # All items in one row\n",
    "             borderaxespad=0)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Create plot for ideology\n",
    "create_demographic_bar_plot(agreement_df, \n",
    "                          llm='llama-3.1-8b-instruct', \n",
    "                          demographic_col='ideology')\n",
    "\n",
    "# Create plot for age\n",
    "create_demographic_bar_plot(agreement_df, \n",
    "                          llm='llama-3.1-8b-instruct', \n",
    "                          demographic_col='age')\n",
    "\n",
    "# Create plot for gender\n",
    "create_demographic_bar_plot(agreement_df, \n",
    "                          llm='llama-3.1-8b-instruct', \n",
    "                          demographic_col='gender')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quote_positions(row: pd.Series) -> Tuple[List[int], List[float], int]:\n",
    "    \"\"\"\n",
    "    Find the starting indices of quotes within the LLM response, normalize them, and count them.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row containing 'llm_response' and 'quotes' columns\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (raw positions, normalized positions, number of quotes)\n",
    "    \"\"\"\n",
    "    response = row['llm_response']\n",
    "    quotes = row['quotes']\n",
    "    response_length = len(response)\n",
    "    \n",
    "    # Handle case where quotes is a string (needs to be evaluated as a list)\n",
    "    if isinstance(quotes, str):\n",
    "        try:\n",
    "            quotes = eval(quotes)  # Safely convert string representation of list to actual list\n",
    "        except:\n",
    "            return [], [], 0\n",
    "    \n",
    "    # Handle empty or missing quotes\n",
    "    if not quotes or not isinstance(quotes, list):\n",
    "        return [], [], 0\n",
    "    \n",
    "    positions = []\n",
    "    for quote in quotes:\n",
    "        try:\n",
    "            pos = response.index(quote)\n",
    "            positions.append(pos)\n",
    "        except ValueError:\n",
    "            # If exact quote not found, try finding best substring match\n",
    "            best_match, match_ratio = find_best_substring_match(response, quote)\n",
    "            if match_ratio > 0.85:  # Using same threshold as before\n",
    "                pos = response.index(best_match)\n",
    "                positions.append(pos)\n",
    "            else:\n",
    "                print(f\"Warning: Could not find quote: '{quote}' in response: '{response}'\")\n",
    "    \n",
    "    # Calculate normalized positions and number of quotes\n",
    "    normalized_positions = [pos/response_length for pos in positions] if positions else []\n",
    "    num_quotes = len(positions)\n",
    "    \n",
    "    return positions, normalized_positions, num_quotes\n",
    "\n",
    "# Apply to DataFrame\n",
    "agreement_df[['quote_positions', 'normalized_quote_positions', 'num_quotes']] = (\n",
    "    agreement_df.apply(find_quote_positions, axis=1)\n",
    "    .apply(pd.Series)  # Convert tuple output to separate columns\n",
    "    .rename(columns={0: 'quote_positions', 1: 'normalized_quote_positions', 2: 'num_quotes'})\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "print(\"Example row:\")\n",
    "sample_row = agreement_df.iloc[0]\n",
    "print(f\"LLM Response: {sample_row['llm_response'][:100]}\")\n",
    "print(f\"Response length: {len(sample_row['llm_response'])}\")\n",
    "print(f\"Quotes: {sample_row['quotes']}\")\n",
    "print(f\"Raw quote positions: {sample_row['quote_positions']}\")\n",
    "print(f\"Normalized quote positions: {sample_row['normalized_quote_positions']}\")\n",
    "print(f\"Number of quotes: {sample_row['num_quotes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quote_position_plot(agreement_df: pd.DataFrame,\n",
    "                             figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing average quote positions across questions for each LLM.\n",
    "    \"\"\"\n",
    "    # Calculate average quote position for each row\n",
    "    agreement_df['avg_quote_position'] = agreement_df['normalized_quote_positions'].apply(\n",
    "        lambda x: np.mean(x) if len(x) > 0 else np.nan\n",
    "    )\n",
    "    \n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    llms = agreement_df['llm'].unique()\n",
    "    questions = agreement_df['question'].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Calculate appropriate bar width\n",
    "    total_width_per_group = 0.8\n",
    "    bar_width = total_width_per_group / len(llms)\n",
    "    group_center_offset = (total_width_per_group - bar_width) / 2\n",
    "    \n",
    "    # Plot bars for each LLM\n",
    "    for idx, llm in enumerate(llms):\n",
    "        llm_data = agreement_df[agreement_df['llm'] == llm]\n",
    "        stats = llm_data.groupby('question')['avg_quote_position'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        position = r + (idx * bar_width) - group_center_offset\n",
    "        ax.bar(position, stats['mean'], bar_width,\n",
    "               yerr=stats['std'], label=llm, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(short_questions, rotation=15, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Average Quote Position (0=start, 1=end)')\n",
    "    ax.set_title('Average Quote Positions by Question and LLM')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Adjust subplot parameters to make room for legend at bottom\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Place legend horizontally at bottom\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.2),\n",
    "             loc='upper center',\n",
    "             ncol=len(llms),\n",
    "             borderaxespad=0)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Plot across LLMs\n",
    "create_quote_position_plot(agreement_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographic_quote_position_plot(agreement_df: pd.DataFrame,\n",
    "                                         llm: str,\n",
    "                                         demographic_col: str,\n",
    "                                         figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing average quote positions across questions,\n",
    "    grouped by a demographic category for a single LLM.\n",
    "    \"\"\"\n",
    "    # Calculate average quote position for each row\n",
    "    agreement_df['avg_quote_position'] = agreement_df['normalized_quote_positions'].apply(\n",
    "        lambda x: np.mean(x) if len(x) > 0 else np.nan\n",
    "    )\n",
    "    \n",
    "    # Filter for specific LLM\n",
    "    llm_data = agreement_df[agreement_df['llm'] == llm]\n",
    "    \n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    questions = llm_data['question'].unique()\n",
    "    demographic_values = llm_data[demographic_col].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Calculate appropriate bar width\n",
    "    total_width_per_group = 0.8\n",
    "    bar_width = total_width_per_group / len(demographic_values)\n",
    "    group_center_offset = (total_width_per_group - bar_width) / 2\n",
    "    \n",
    "    # Plot bars for each demographic value\n",
    "    for idx, demo_value in enumerate(demographic_values):\n",
    "        demo_data = llm_data[llm_data[demographic_col] == demo_value]\n",
    "        stats = demo_data.groupby('question')['avg_quote_position'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        position = r + (idx * bar_width) - group_center_offset\n",
    "        ax.bar(position, stats['mean'], bar_width,\n",
    "               yerr=stats['std'], label=demo_value, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(short_questions, rotation=15, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Average Quote Position (0=start, 1=end)')\n",
    "    ax.set_title(f'Average Quote Positions by Question and {demographic_col}\\nfor {llm}')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Adjust subplot parameters to make room for legend at bottom\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Place legend horizontally at bottom\n",
    "    ax.legend(title=demographic_col,\n",
    "             bbox_to_anchor=(0.5, -0.2),\n",
    "             loc='upper center',\n",
    "             ncol=len(demographic_values),\n",
    "             borderaxespad=0)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Plot for specific LLM and demographic\n",
    "create_demographic_quote_position_plot(agreement_df,\n",
    "                                     llm='llama-3.1-8b-instruct',\n",
    "                                     demographic_col='ideology')\n",
    "\n",
    "create_demographic_quote_position_plot(agreement_df,\n",
    "                                     llm='gpt-4o-mini',\n",
    "                                     demographic_col='ideology')\n",
    "\n",
    "create_demographic_quote_position_plot(agreement_df,\n",
    "                                     llm='gpt-3.5-turbo',\n",
    "                                     demographic_col='ideology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quote_real_estate(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of the response that is quoted.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row containing 'llm_response' and 'quotes' columns\n",
    "        \n",
    "    Returns:\n",
    "        Float representing proportion of response that is quoted (0-1)\n",
    "    \"\"\"\n",
    "    response = row['llm_response']\n",
    "    quotes = row['quotes']\n",
    "    \n",
    "    # Handle case where quotes is a string\n",
    "    if isinstance(quotes, str):\n",
    "        try:\n",
    "            quotes = eval(quotes)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    # Handle empty or missing quotes\n",
    "    if not quotes or not isinstance(quotes, list):\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate total length of quotes\n",
    "    total_quote_length = sum(len(quote) for quote in quotes)\n",
    "    response_length = len(response)\n",
    "    \n",
    "    return total_quote_length / response_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_real_estate_plot(agreement_df: pd.DataFrame,\n",
    "                          figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing quote real estate across questions for each LLM.\n",
    "    \"\"\"\n",
    "    # Calculate real estate for each row\n",
    "    agreement_df['quote_real_estate'] = agreement_df.apply(calculate_quote_real_estate, axis=1)\n",
    "    \n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    llms = agreement_df['llm'].unique()\n",
    "    questions = agreement_df['question'].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Calculate appropriate bar width\n",
    "    total_width_per_group = 0.8\n",
    "    bar_width = total_width_per_group / len(llms)\n",
    "    group_center_offset = (total_width_per_group - bar_width) / 2\n",
    "    \n",
    "    # Plot bars for each LLM\n",
    "    for idx, llm in enumerate(llms):\n",
    "        llm_data = agreement_df[agreement_df['llm'] == llm]\n",
    "        stats = llm_data.groupby('question')['quote_real_estate'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        position = r + (idx * bar_width) - group_center_offset\n",
    "        ax.bar(position, stats['mean'], bar_width,\n",
    "               yerr=stats['std'], label=llm, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(short_questions, rotation=15, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Quote Real Estate (proportion of response quoted)')\n",
    "    ax.set_title('Proportion of Response Quoted by Question and LLM')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Adjust subplot parameters to make room for legend at bottom\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Place legend horizontally at bottom\n",
    "    ax.legend(bbox_to_anchor=(0.5, -0.2),\n",
    "             loc='upper center',\n",
    "             ncol=len(llms),\n",
    "             borderaxespad=0)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Plot across LLMs\n",
    "create_real_estate_plot(agreement_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_demographic_real_estate_plot(agreement_df: pd.DataFrame,\n",
    "                                      llm: str,\n",
    "                                      demographic_col: str,\n",
    "                                      figsize: Tuple[int, int] = (15, 8)) -> None:\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing quote real estate across questions,\n",
    "    grouped by a demographic category for a single LLM.\n",
    "    \"\"\"\n",
    "    # Calculate real estate for each row\n",
    "    agreement_df['quote_real_estate'] = agreement_df.apply(calculate_quote_real_estate, axis=1)\n",
    "    \n",
    "    # Filter for specific LLM\n",
    "    llm_data = agreement_df[agreement_df['llm'] == llm]\n",
    "    \n",
    "    # Setup\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    setup_plot_style(fig, ax)\n",
    "    \n",
    "    # Get unique values\n",
    "    questions = llm_data['question'].unique()\n",
    "    demographic_values = llm_data[demographic_col].unique()\n",
    "    r = np.arange(len(questions))\n",
    "    \n",
    "    # Calculate appropriate bar width\n",
    "    total_width_per_group = 0.8\n",
    "    bar_width = total_width_per_group / len(demographic_values)\n",
    "    group_center_offset = (total_width_per_group - bar_width) / 2\n",
    "    \n",
    "    # Plot bars for each demographic value\n",
    "    for idx, demo_value in enumerate(demographic_values):\n",
    "        demo_data = llm_data[llm_data[demographic_col] == demo_value]\n",
    "        stats = demo_data.groupby('question')['quote_real_estate'].agg(['mean', 'std']).reset_index()\n",
    "        \n",
    "        position = r + (idx * bar_width) - group_center_offset\n",
    "        ax.bar(position, stats['mean'], bar_width,\n",
    "               yerr=stats['std'], label=demo_value, alpha=0.7, capsize=5)\n",
    "    \n",
    "    # Customize plot\n",
    "    short_questions = [q[:30] + '...' for q in questions]\n",
    "    ax.set_xticks(r)\n",
    "    ax.set_xticklabels(short_questions, rotation=15, ha='right')\n",
    "    ax.set_xlabel('Questions')\n",
    "    ax.set_ylabel('Quote Real Estate (proportion of response quoted)')\n",
    "    ax.set_title(f'Proportion of Response Quoted by Question and {demographic_col}\\nfor {llm}')\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Adjust subplot parameters to make room for legend at bottom\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    \n",
    "    # Place legend horizontally at bottom\n",
    "    ax.legend(title=demographic_col,\n",
    "             bbox_to_anchor=(0.5, -0.2),\n",
    "             loc='upper center',\n",
    "             ncol=len(demographic_values),\n",
    "             borderaxespad=0)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Plot for specific LLM and demographic\n",
    "create_demographic_real_estate_plot(agreement_df,\n",
    "                                  llm='llama-3.1-8b-instruct',\n",
    "                                  demographic_col='ideology')\n",
    "\n",
    "create_demographic_real_estate_plot(agreement_df,\n",
    "                                  llm='gpt-4o-mini',\n",
    "                                  demographic_col='ideology')\n",
    "\n",
    "create_demographic_real_estate_plot(agreement_df,\n",
    "                                  llm='gpt-3.5-turbo',\n",
    "                                  demographic_col='ideology')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
