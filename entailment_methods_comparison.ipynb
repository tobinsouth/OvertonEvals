{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Entailment\n",
    "\n",
    "How do we know if one text response matches the opinions in another response? This is a tricky question, and there are many ways to approach it. This notebook is going to run through a series of approaches and models, to do some meta-analysis on the best way to do this.\n",
    "\n",
    "It will depend on the `habermas_machine_questions_with_responses.csv` file, which contains the questions and responses generated by the LLMs, as well as the human responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv')\n",
    "df_questions['perspectives'] = df_questions['perspectives'].apply(ast.literal_eval)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at a random question to test the different methods\n",
    "example = df_questions.sample(1).iloc[0]\n",
    "print(\"Question: \", example['question'], \"\\n\", \"Response: \\n\", example['gpt-3.5-turbo'], \"\\n\\n\", \"Opinion: \\n\", example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a constructed example to test the different methods.\n",
    "strawman_question = \"Should we ban or limit access to guns?\"\n",
    "strawman_response_easy = \"No, we should not ban guns. Guns are a necessary tool for self-defense and hunting. We should also make sure that we have a strong police force to protect people and their property.\"\n",
    "strawman_response_hard = \"No, we should not ban guns completely, but we should make it harder for people to get them with strong laws limiting who can buy them.\"\n",
    "strawman_response_mixed = \"While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.\"\n",
    "strawman_opinion = \"We should ban guns to reduce crime.\"\n",
    "strawman_opinion_2 = \"Guns are a fundamental right!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Representation\n",
    "\n",
    "These functions are used to determine if the LLM responses match the human opinions (not entailment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, a dead simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to see if the opinions are all represented in the generated responses.\n",
    "def is_my_opinion_represented(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the following opinion is represented in the response to a question. This is the simplest approach I can think of.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Task: Determine if the following opinion is represented in the response to a question.\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Instructions:\n",
    "- Answer ONLY with 'yes' or 'no'\n",
    "- Answer 'yes' if the opinion is clearly represented\n",
    "- Answer 'no' if the opinion is absent\n",
    "- Do not explain your reasoning\n",
    "- Do not add any other text\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise evaluator that only responds with 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        max_tokens=1, # We only need one token for 'yes' or 'no'\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_my_opinion_represented(example['question'], example['gpt-4o-mini'], example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's use a structured CoT to evaluate the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class OpinionPoint(BaseModel):\n",
    "    claim: str\n",
    "    explicit_matches: list[str]\n",
    "    implicit_matches: list[str]\n",
    "    contradictions: list[str]\n",
    "    coverage_score: int  # 0-10 for this specific point\n",
    "\n",
    "class EvaluationStep(BaseModel):\n",
    "    step_number: int\n",
    "    analysis: str\n",
    "    findings: list[str]\n",
    "\n",
    "class RepresentationAnalysis(BaseModel):\n",
    "    opinion_points: list[OpinionPoint]\n",
    "    evaluation_steps: list[EvaluationStep]\n",
    "    final_score: int  # 0-10 overall score\n",
    "    reasoning: str  # Brief explanation of final score\n",
    "\n",
    "def is_my_opinion_represented_structured_cot(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the opinion is represented in the response to a question, using structured CoT generation.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Evaluate how well an opinion is represented in a response through careful step-by-step analysis.\n",
    "\n",
    "Follow these specific steps in your evaluation:\n",
    "1. First, break down the core claims/points in the opinion\n",
    "2. For each point in the opinion:\n",
    "   - Search for explicit mentions in the response\n",
    "   - Look for implicit/paraphrased representations\n",
    "   - Note any contradictions\n",
    "3. Consider the overall alignment:\n",
    "   - How many points are covered?\n",
    "   - How directly are they addressed?\n",
    "   - Are there any misalignments?\n",
    "4. Score the representation from 0-10 where:\n",
    "   - 0: Complete contradiction or no representation\n",
    "   - 1-3: Minimal/weak representation of few points\n",
    "   - 4-6: Partial representation of main points\n",
    "   - 7-9: Strong representation of most points\n",
    "   - 10: Complete and explicit representation of all points\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Analyze step-by-step following the instructions, then provide your structured evaluation.\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"RepresentationChain\", \n",
    "                \"schema\": RepresentationAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "    \n",
    "    result_object = json.loads(completion.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_representation_result(result_object):\n",
    "    try:\n",
    "        return result_object['final_score']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how well the LLMs are doing at finding the opinion in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_my_opinion_represented_structured_cot(example['question'], example['gpt-4o-mini'], example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment\n",
    "\n",
    "Entailment is a lot trickier than representation. Our approach is going to be to use the model to find the exact text that matches the opinion, and then we'll see if that text is in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# # Generate colors using matplotlib's color map\n",
    "# n_colors = len(spans)\n",
    "# color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "# colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "\n",
    "# # Generate a style sheet for the spans\n",
    "# style_sheet = \"<style>\\n\"\n",
    "# for perspective_id, _ in enumerate(spans):\n",
    "#     style_sheet += f\"\"\".highlight-{perspective_id} {{ position: relative; }}\n",
    "#     .highlight-{perspective_id}::after {{\n",
    "#         content: \"\";\n",
    "#         position: absolute;\n",
    "#         left: 0; right: 0;\n",
    "#         bottom: -{2*perspective_id}px; /* offset slightly below baseline */\n",
    "#         border-bottom: 2px solid {colors[perspective_id]};\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "# style_sheet += \"</style>\"\n",
    "\n",
    "# # Convert dict to list of (start, end, color) and sort by start position\n",
    "# ordered_span = [(start, end, perspective_id) \n",
    "#                  for perspective_id, span_list in enumerate(spans)\n",
    "#                  for start, end in span_list]\n",
    "# ordered_span.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# last_idx = 0\n",
    "\n",
    "# for span_idx, (start, end, perspective_id) in enumerate(ordered_span):\n",
    "#     # Add text before the span\n",
    "#     result.append(text[last_idx:start])\n",
    "#     # Add the highlighted span\n",
    "#     result.append(f\"<span class='highlight-{perspective_id}'>\")\n",
    "#     if span_idx+1 < len(ordered_span) and ordered_span[span_idx+1][0] < end:\n",
    "#         result.append(text[start:end])\n",
    "#     else:\n",
    "#         result.append(text[start:end])\n",
    "#     result.append(\"</span>\")\n",
    "#     last_idx = end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_spans(text: str, spans: list[list[tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Highlight the spans in the text based on the spans dictionary. \n",
    "    Args:\n",
    "        text: The text to highlight\n",
    "        # spans_dict: Dictionary of 'color': list of spans, where each span is (start, end)\n",
    "    Example:\n",
    "        spans = [\n",
    "            [(0, 4), (10, 16)],\n",
    "            [(28, 37)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    from IPython.display import Markdown, display\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Generate colors using matplotlib's color map\n",
    "    n_colors = len(spans)\n",
    "    color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "    colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "    # Create the spans dictionary using these colors\n",
    "    spans_dict = {color: spans[i] for i, color in enumerate(colors)}\n",
    "    # highlight_spans(text, spans_dict)\n",
    "\n",
    "    # Generate a style sheet for the spans\n",
    "    style_sheet = \"<style>\"\n",
    "    for span_id, span in enumerate(spans):\n",
    "        style_sheet += f\"\"\".highlight-{span_id} {{ background-color: {colors[span_id]}; }}\n",
    "        .highlight-{span_id}::after {{\n",
    "            content: \"\";\n",
    "            position: absolute;\n",
    "            left: 0; right: 0;\n",
    "            bottom: -{2*span_id}px; /* offset slightly below baseline */\n",
    "            border-bottom: 2px solid {colors[span_id]};\n",
    "        }}\n",
    "        \"\"\"\n",
    "    style_sheet += \"</style>\"\n",
    "    \n",
    "    # Convert dict to list of (start, end, color) and sort by start position\n",
    "    all_spans = [(start, end, color) \n",
    "                 for color, spans in spans_dict.items() \n",
    "                 for start, end in spans]\n",
    "    all_spans.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Build the marked up text piece by piece\n",
    "    result = []\n",
    "    last_idx = 0\n",
    "    \n",
    "    for start, end, color in all_spans:\n",
    "        # Add text before the span\n",
    "        result.append(text[last_idx:start])\n",
    "        # Add the highlighted span\n",
    "        result.append(f\"<span style='color: {color};'>{text[start:end]}</span>\")\n",
    "        last_idx = end\n",
    "    \n",
    "    # Add any remaining text after the last span\n",
    "    result.append(text[last_idx:])\n",
    "    \n",
    "    # Add color swatches at the end\n",
    "    for color in spans_dict:\n",
    "        swatch = f\"<span style='color: {color};'>{chr(9608) * 6}</span>\"\n",
    "        result.append(f\" {swatch}\")\n",
    "    \n",
    "    # Join all pieces and display\n",
    "    marked_text = ''.join(result)\n",
    "    display(Markdown(marked_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured CoT (OpenAI) for entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the writefile magic to save the entailment code to a file for use downsteam and for version control.\n",
    "# %%writefile src/entailment.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import openai, os, json\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "class EntailmentMatch(BaseModel):\n",
    "    text: str\n",
    "    match_type: str  # \"direct\", \"paraphrase\", or \"contextual\"\n",
    "    confidence: int  # 0-10 score\n",
    "    explanation: str  # Why this is a match\n",
    "\n",
    "class EntailmentStep(BaseModel):\n",
    "    step_number: int\n",
    "    concept: str  # The concept from the opinion being analyzed\n",
    "    analysis: str  # The reasoning process\n",
    "    matches: list[EntailmentMatch]\n",
    "\n",
    "class EntailmentAnalysis(BaseModel):\n",
    "    steps: list[EntailmentStep]\n",
    "    final_matches: list[str]  # The best, most confident matches\n",
    "    coverage_score: int  # 0-10 how well the opinion is covered\n",
    "\n",
    "def entailment_from_gpt_json(question: str, response: str, opinion: str, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Find exact text matches between rich text and opinion using GPT-4.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Precise Text Entailment Analysis. Find and evaluate text in the Response that represents concepts from the Opinion.\n",
    "\n",
    "Follow these specific steps:\n",
    "1. Break down the Opinion into key concepts\n",
    "2. For each concept:\n",
    "   - Search for direct text matches, this includes single words like \"yes\" or \"no\"\n",
    "   - Identify paraphrased representations\n",
    "   - Look for contextual/implicit matches\n",
    "   - Copy the **exact text** in the Response that matches the concept in the Opinion. Copy the text from the response, not the opinion.\n",
    "\n",
    "3. Evaluate matches by:\n",
    "   - Precision: How exactly does it match?\n",
    "   - Context: Is the meaning preserved?\n",
    "   - Completeness: Is the full concept captured?\n",
    "\n",
    "4. Score coverage from 0-10 where:\n",
    "   - 0: No valid matches found\n",
    "   - 1-3: Few weak/partial matches\n",
    "   - 4-6: Some good matches but incomplete\n",
    "   - 7-9: Strong matches for most concepts\n",
    "   - 10: Complete, precise matches for all concepts\n",
    "\n",
    "Important:\n",
    "- Prioritize precision over quantity\n",
    "- Consider context to avoid false matches\n",
    "- Explain reasoning for each match\n",
    "- Always copy the exact text from the Response that matches the concept\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Context question: {question}\n",
    "Opinion: {opinion}\n",
    "Response: {response}\n",
    "\n",
    "Analyze step-by-step following the instructions to find and evaluate all relevant matches.\"\"\"\n",
    "    \n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"EntailmentAnalysis\", \n",
    "                \"schema\": EntailmentAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "\n",
    "    result_object = json.loads(chat_response.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_entailment_result(result_object, response):\n",
    "    matches = []\n",
    "    for match in result_object['final_matches']:\n",
    "        start_index = response.lower().find(match.lower())\n",
    "        if start_index == -1:\n",
    "            print(\"Warning: match was not found in response text.\")\n",
    "            continue\n",
    "        end_index = start_index + len(match)\n",
    "        matches.append((start_index, end_index))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_results = entailment_from_gpt_json(example['question'], example['gpt-3.5-turbo'], example['perspectives'][0])\n",
    "entailment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_matches = process_entailment_result(entailment_results, example['gpt-3.5-turbo'])\n",
    "print(entailment_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strawman_entailment_results = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion)\n",
    "# strawman_entailment_matches = process_entailment_result(strawman_entailment_results, strawman_response_mixed)\n",
    "# print(strawman_entailment_matches)\n",
    "\n",
    "strawman_entailment_results_2 = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion_2)\n",
    "strawman_entailment_matches_2 = process_entailment_result(strawman_entailment_results_2, strawman_response_mixed)\n",
    "print(strawman_entailment_matches_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_spans(example['gpt-3.5-turbo'], [entailment_matches])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to run this across all the opinions and highlight each match in the response.\n",
    "\n",
    "opinion_entailments = []\n",
    "for opinion_idx, opinion in enumerate(tqdm(example['perspectives'])):\n",
    "    opinion_entailments.append({})\n",
    "    opinion_entailments[opinion_idx]['full_result'] = entailment_from_gpt_json(example['question'], example['gpt-4o-mini'], opinion)\n",
    "    opinion_entailments[opinion_idx]['matches'] = process_entailment_result(opinion_entailments[opinion_idx]['full_result'], example['gpt-4o-mini'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the matches for the example\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Generate colors using matplotlib's color map\n",
    "n_colors = len(opinion_entailments)\n",
    "color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "# Create the spans dictionary using these colors\n",
    "spans_dict = {color: opinion_entailments[i]['matches'] for i, color in enumerate(colors)}\n",
    "highlight_spans(example['gpt-4o-mini'], list(spans_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MetaAI SONAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sonar-space\n",
    "!pip install fairseq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "# t2vec_model = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\",\n",
    "#                                            tokenizer=\"text_sonar_basic_encoder\")\n",
    "# sentences = ['My name is SONAR.', 'I can embed the sentences into vectorial space.']\n",
    "# embeddings = t2vec_model.predict(sentences, source_lang=\"eng_Latn\")\n",
    "# print(embeddings.shape)\n",
    "# # torch.Size([2, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using real NLI methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch\n",
    "# !pip install sentence-splitter\n",
    "# !pip install sentence-transformers\n",
    "# !pip install tiktoken\n",
    "# !pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(strawman_response_mixed)\n",
    "print(strawman_opinion)\n",
    "input = tokenizer(strawman_response_mixed, strawman_opinion_2, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "text = \"This is a paragraph. It contains several sentences. \\\"But why,\\\" you ask?\"\n",
    "sentences = split_text_into_sentences(text=text, language='en')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We could also validate with facebook/xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge-mnli\")\n",
    "NLI_model = transformers.pipeline(\"text-classification\", model=\"microsoft/deberta-v2-xlarge-mnli\", tokenizer=tokenizer, top_k=None, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strawman_response_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"text\": example['gpt-3.5-turbo'], \"text_pair\": example['own_opinion.text'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-comparing the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I apologize for how chaotic and unreadable this cell is, but we want to run all the same functions and save all intermediate results for debugging.\n",
    "\n",
    "entailment_models = ['gpt-4o-mini']\n",
    "response_models = ['gpt-3.5-turbo']\n",
    "\n",
    "overton_results = []\n",
    "sample_size = 3\n",
    "for _, row in tqdm(df_questions.sample(sample_size).iterrows(), total=sample_size, desc=\"Questions\", leave=True):\n",
    "    question = row['question.text']\n",
    "    opinions = row['own_opinion.text']\n",
    "    question_id = row['question_id']\n",
    "    with tqdm(total=len(opinions), desc=\"Opinions\", leave=False) as opinion_bar:\n",
    "        for opinion_idx, opinion in enumerate(opinions):\n",
    "            with tqdm(total=len(response_models), desc=\"Response Models\", leave=False) as response_bar:\n",
    "                for response_model in response_models:\n",
    "                    with tqdm(total=len(entailment_models), desc=\"Entailment Models\", leave=False) as entailment_bar:\n",
    "                        for entailment_model in entailment_models:\n",
    "                            response = row[response_model]\n",
    "                            # This is the simple prompt check (which should be the same for all models)\n",
    "                            result_opinion_represented = is_my_opinion_represented(question, response, opinion, model=entailment_model)\n",
    "                            # This is the structured cot check, which should be the same for all models\n",
    "                            result_opinion_represented_structured_cot = is_my_opinion_represented_structured_cot(question, response, opinion, model=entailment_model)\n",
    "                            result_opinion_represented_structured_cot_score = process_representation_result(result_opinion_represented_structured_cot)\n",
    "\n",
    "\n",
    "                            # Now we turn to entailment, which we're going to save in a bunch of different formats for debugging\n",
    "                            entailment_result = entailment_from_gpt_json(question, response, opinion, model=entailment_model)\n",
    "                            entailment_matches = process_entailment_result(entailment_result, response)\n",
    "\n",
    "                            overton_results.append({\n",
    "                                'question_id': question_id,\n",
    "                                'opinion_idx': opinion_idx,\n",
    "                                'response_model': response_model,\n",
    "                                'entailment_model': entailment_model,\n",
    "                                'is_represented_simple_prompt': result_opinion_represented == 'yes',\n",
    "                                'is_represented_structured_cot': result_opinion_represented_structured_cot,\n",
    "                                'is_represented_structured_cot_score': result_opinion_represented_structured_cot_score,\n",
    "                                'entailment_result': entailment_result,\n",
    "                                'entailment_matches': entailment_matches\n",
    "                            })\n",
    "                            entailment_bar.update(1)\n",
    "                    response_bar.update(1)\n",
    "            opinion_bar.update(1)\n",
    "\n",
    "df_overton_results = pd.DataFrame(overton_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.DataFrame(overton_results)\n",
    "df_overton_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results.to_csv('data/entailment_ablation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.read_csv('data/entailment_ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results.groupby(['model', 'question_id'])['is_represented_simple_prompt'].mean().reset_index().groupby('model')['is_represented_simple_prompt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results['is_represented_structured_cot_score.bool'] = df_overton_results['is_represented_structured_cot_score'] > 5\n",
    "df_overton_results['meta_analysis.simpleXstructured'] = (df_overton_results['is_represented_simple_prompt'] == df_overton_results['is_represented_structured_cot_score.bool']).astype(int)\n",
    "df_overton_results['meta_analysis.entailmentLength'] = df_overton_results['entailment_matches'].apply(lambda x: len(x))\n",
    "df_overton_results['meta_analysis.entailmentXstructured'] = ((df_overton_results['meta_analysis.entailmentLength'] > 1) == df_overton_results['is_represented_structured_cot_score.bool'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Meta analysis:\n",
    "- Percent represented (structured cot) > 5: {df_overton_results['is_represented_structured_cot_score.bool'].mean()}\n",
    "- Percent represented (simple prompt) == Percent represented (structured cot): {df_overton_results['meta_analysis.simpleXstructured'].mean()}\n",
    "- Percent entailment length > 1 == represented (structured cot): {df_overton_results['meta_analysis.entailmentXstructured'].mean()}\n",
    "- Mean Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].mean()}\n",
    "- Max Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =df_overton_results.sample(1).iloc[0]\n",
    "example_question = df_questions.loc[df_questions['question_id'] == example['question_id']].iloc[0]\n",
    "opinion = example_question['own_opinion.text'][example['opinion_idx']]\n",
    "response =example_question[example['response_model']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Question: {example_question['question.text']}\n",
    "      \n",
    "Response: {response}\n",
    "\n",
    "From model: {example['response_model']}\n",
    "\n",
    "Opinion: {opinion}\n",
    "\n",
    "ANALYSIS:\n",
    "\n",
    "Is represented (simple prompt): {example['is_represented_simple_prompt']}\n",
    "\n",
    "Is represented (structured cot): {example['is_represented_structured_cot']}\n",
    "\n",
    "Entailment result: {example['entailment_matches']}\n",
    "\n",
    "Entailment Reasoning: {example['entailment_result']}\n",
    "\n",
    "META ANALYSIS:\n",
    "\n",
    "Simple prompt == Structured cot: {example['meta_analysis.simpleXstructured'] == 1}\n",
    "\n",
    "Entailment length > 1 AND simple prompt == structured cot: {example['meta_analysis.entailmentXstructured']}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
