{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Vanilla LLM Responses\n",
    "### We're going to generate responses from a LLM for each question, based on the habermas_machine_questions.csv file.\n",
    "\n",
    "The input is the habermas_machine_questions.csv file.\n",
    "The output is a large CSV of questions, opinions, and LLM responses. LLM responses & questions are 1-to-1 but duplicated across varying opinions. This is not space efficient but makes it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives.csv')\n",
    "if 'Unnamed: 0' in df_questions.columns:\n",
    "    df_questions.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(\"df_questions.shape: \", df_questions.shape)\n",
    "df_questions.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the number of questions to 100 for testing with an equal weight for each source. \n",
    "sample_size = 100\n",
    "print(\"Sampling \", sample_size, \" questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_deliberation = df_questions['source'] == 'deliberation'\n",
    "sample = df_questions[~is_deliberation].groupby('source').sample(sample_size//3, random_state=42).reset_index(drop=True)\n",
    "df_questions = pd.concat([sample, df_questions[is_deliberation]]).reset_index(drop=True)\n",
    "print(df_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def generate_responses(questions, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    \"\"\"\n",
    "    print(\"Generating responses for: \", output_path)\n",
    "    \n",
    "    # Load existing responses if any and if we want to resume\n",
    "    responses = {}\n",
    "    if start_from_checkpoint:\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Get questions that haven't been answered yet for this model\n",
    "    remaining_questions = [\n",
    "        q for q in questions \n",
    "        if q not in responses\n",
    "    ]\n",
    "        \n",
    "    if not remaining_questions:\n",
    "        print(f\"All questions already processed.\")\n",
    "        return\n",
    "                \n",
    "    # Process each remaining question with progress bar\n",
    "    for idx, question in enumerate(tqdm(remaining_questions, desc=f\"Generating responses\", smoothing=0, ascii=True)):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generation_function(question)\n",
    "            \n",
    "            # Store response\n",
    "            responses[question] = response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing question '{question}' for: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            # Save to JSON\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(responses, f, indent=2)\n",
    "                \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running questions through various LLMs\n",
    "\n",
    "\n",
    "### OpenAI Models\n",
    "We're going to start with OpenAI models. You'll need to set your OpenAI API key in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate an LLM response for each question, for each AI model.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_openai_response(question, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# OpenAI models\n",
    "oai_models = ['gpt-4o-mini','gpt-3.5-turbo']\n",
    "\n",
    "for model in oai_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    generation_function = lambda x: generate_openai_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingace Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from huggingface_hub import InferenceClient\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "def query_huggingface(hf_client: InferenceClient, inputs: str, chat:bool=False) -> str:\n",
    "    \"\"\"\n",
    "    This is a helper function to query the Huggingface API.\n",
    "\n",
    "    Huggingface models are either simple inference endpoints, dedicated endpoints, or chat endpoints. We can use dedicated endpoints by passing in the API URL directly.\n",
    "\n",
    "    For non-chat models, we use the text_generation endpoint over the chat_completion endpoint.\n",
    "    \"\"\"\n",
    "    if chat:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": inputs\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        completion =  hf_client.chat_completion(\n",
    "            messages=messages, \n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        completion =  hf_client.text_generation(\n",
    "            prompt=inputs, \n",
    "            max_new_tokens=500\n",
    "        )\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because many of the non-instruction tuned based models don't have warm inference endpoints (and aren't available on tools like Together), we're going to create dedicated endpoints on AWS through the Huggingface API.\n",
    "# The dedicated endpoints take a while to warm up (also you may need to fiddle with URLs)\n",
    "\n",
    "hf_models = {\n",
    "    # 'gemma-2-2b-it': \"https://z70frgvzih3230r1.us-east-1.aws.endpoints.huggingface.cloud\", \n",
    "    # 'gemma-2-2b': \"https://vbkt0rabiunjn175.us-east-1.aws.endpoints.huggingface.cloud\",  \n",
    "    # \"llama-3.1-8B\": \"https://ropkydxq3vq8qff9.us-east-1.aws.endpoints.huggingface.cloud\", # These will be slow to run.\n",
    "    # \"llama-3.1-8B-it\": \"https://dxdj0n50tbi0mgar.us-east-1.aws.endpoints.huggingface.cloud\", \n",
    "}\n",
    "\n",
    "non_chat_models = ['llama-3.1-8B', 'gemma-2-2b-it', 'gemma-2-2b']\n",
    "\n",
    "for model, api_url in hf_models.items():\n",
    "    output_file = model+'_responses.json'\n",
    "    hf_client = InferenceClient(model=api_url, token=hf_api_key)\n",
    "    if model in non_chat_models:\n",
    "        generation_function = lambda x: query_huggingface(hf_client, x, chat=False)\n",
    "    else:\n",
    "        generation_function = lambda x: query_huggingface(hf_client, x, chat=True)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna run this with Google Deepmind Models. You may need to run:\n",
    "\n",
    "`gcloud components update`\n",
    "\n",
    "`gcloud auth application-default login`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# Get the project ID from the .env file\n",
    "PROJECT_ID = os.getenv('GOOGLE_PROJECT_ID')\n",
    "print(\"Running with PROJECT_ID: \", PROJECT_ID)\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "gdp_models = ['gemini-1.5-flash-002']\n",
    "for model in gdp_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    model = GenerativeModel(model)\n",
    "    generation_function = lambda x: model.generate_content(x).text\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Together AI Models\n",
    "\n",
    "Together is a great model provider but doesn't have a lot of non-instruction tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a prompt suggested by Michiel to few shot non-instruction tuned models.\n",
    "# michiel_prompt = lambda x: f\"\"\"Q: What is an ambigram\n",
    "# A: An ambigram is a word, phrase, symbol, or design that retains its meaning or readability when viewed from a different perspective, orientation, or transformation. It is a type of visual wordplay that often relies on symmetry, rotation, or reflection.\n",
    "\n",
    "# Q: What are the zipcodes in Cambridge MA?\n",
    "# A: Cambridge, MA, has multiple ZIP codes depending on the specific area. Here are the primary ones:\n",
    "# 02138: Harvard Square and surrounding areas\n",
    "# 02139: Central Square and parts of MIT\n",
    "# 02140: Porter Square and North Cambridge\n",
    "# 02141: East Cambridge\n",
    "# 02142: Kendall Square and parts of MIT\n",
    "# If you need a ZIP code for a specific address or neighborhood, let me know!\n",
    "\n",
    "# Q: What country is Steve Irwin from?\n",
    "# A: Steve Irwin, famously known as \"The Crocodile Hunter,\" was from Australia. Born in Essendon, Victoria, Australia, he was an internationally renowned wildlife expert, conservationist, and television personality known for his passion for wildlife and environmental preservation.\n",
    "\n",
    "# Q: {x}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, model):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": question}],\n",
    "      max_tokens=2048\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# While you should be able to use the mistral models on HF, together is much faster with a dedicated endpoint and more models.\n",
    "together_models = {\n",
    "    'mistral-7b-instruct': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    # 'mistral-7b': 'mistralai/Mistral-7B-v0.1',\n",
    "    'llama-3.1-8b-instruct': 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo-128K',\n",
    "    'gemma-2b-it': 'google/gemma-2b-it'\n",
    "}\n",
    "\n",
    "for bettername, model in together_models.items():\n",
    "    output_file = bettername+'_responses.json'\n",
    "\n",
    "    if bettername == 'mistral-7b':\n",
    "       nonITprompt = lambda x: f\"Question: {x}\\n\\nAnswer:\"\n",
    "    #    nonITprompt = lambda x: michiel_prompt(x)\n",
    "       generation_function = lambda x: generate_together_response(nonITprompt(x), model)\n",
    "    else:\n",
    "        generation_function = lambda x: generate_together_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Responses\n",
    "#### We're now going to load in all the responses and make them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives.csv') # Load in without and sampling or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = oai_models + gdp_models + list(together_models.keys()) + list(hf_models.keys())\n",
    "\n",
    "for model in all_models:\n",
    "    with open(TEMP_PATH+model+'_responses.json', 'r') as f:\n",
    "        model_responses = json.load(f)\n",
    "        df_questions[model] = df_questions['question'].map(model_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.dropna(inplace=True) # Only keep rows where all models have responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to do a basic check to make sure all the model columns are non-null.\n",
    "assert df_questions.isnull().sum().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv')\n",
    "df_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
