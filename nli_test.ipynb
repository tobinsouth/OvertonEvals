{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out NLI \n",
    "\n",
    "Notebook to get the baseline usage of the model `tals/albert-xlarge-vitaminc` from [HuggingFace](https://huggingface.co/tals/albert-xlarge-vitaminc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few text examples for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_claim_pairs = [\n",
    "    {\n",
    "        \"evidence\": \"The new policy has led to a significant decrease in crime rates.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Supports\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"There are no studies showing a direct link between the policy and crime rates.\",\n",
    "        \"claim\": \"The new policy has a high impact on crime rates.\",\n",
    "        \"label\": \"Not enough info\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"Crime rates have increased since the policy was implemented.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Refutes\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction pipeline\n",
    "\n",
    "Takes in two strings, evidence and claim. Outputs the tuple of the label and corresponding prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text_a, text_b):\n",
    "    \"\"\"Predicts the relationship between a claim and evidence. \n",
    "\n",
    "    Args:\n",
    "        text_a (str): The evidence statement.\n",
    "        text_b (str): The claim statement.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted relationship label (\"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\").\n",
    "        float: The confidence score of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        text_a, text_b,\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        padding=True,         # Pad to the longest sequence in the batch\n",
    "        truncation=True       # Truncate to the model's max length\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class and its score\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_score = probabilities[0, predicted_class].item()\n",
    "\n",
    "    # Label mapping\n",
    "    label_map = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT ENOUGH INFO\"}  # Updated label mapping\n",
    "\n",
    "    return label_map[predicted_class], predicted_score  # Return label and score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silly example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "label, score = predict_label(\"The sky is blue.\", \"The color of the pineapple is orange.\")\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, score = predict_label(\n",
    "    \"The sky is light blue.\", # evidence\n",
    "    \"The color of the sky is blue.\" # claim\n",
    "    )\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Lets try for our original examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in evidence_claim_pairs:\n",
    "    print(\n",
    "        predict_label(pair['evidence'],pair['claim']))\n",
    "    print(\"ground truth:\", pair['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Yay! \n",
    "\n",
    "Lets move onto working this for value kaleidescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
