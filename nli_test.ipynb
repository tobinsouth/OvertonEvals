{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out NLI \n",
    "\n",
    "Notebook to get the baseline usage of the model `tals/albert-xlarge-vitaminc` from [HuggingFace](https://huggingface.co/tals/albert-xlarge-vitaminc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few text examples for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_claim_pairs = [\n",
    "    {\n",
    "        \"evidence\": \"The new policy has led to a significant decrease in crime rates.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Supports\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"There are no studies showing a direct link between the policy and crime rates.\",\n",
    "        \"claim\": \"The new policy has a high impact on crime rates.\",\n",
    "        \"label\": \"Not enough info\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"Crime rates have increased since the policy was implemented.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Refutes\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/entailment_vitc.py \n",
    "# This will write the function to a file for use downstream in the pipeline. It can be commented out while experimenting.\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction pipeline\n",
    "\n",
    "Takes in two strings, evidence and claim. Outputs the tuple of the label and corresponding prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "def NLI(text_a, text_b):\n",
    "    \"\"\"Predicts the relationship between a claim and evidence. \n",
    "\n",
    "    Args:\n",
    "        text_a (str): The evidence statement.\n",
    "        text_b (str): The claim statement.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted relationship label (\"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\").\n",
    "        float: The confidence score of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        text_a, text_b,\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        padding=True,         # Pad to the longest sequence in the batch\n",
    "        truncation=True       # Truncate to the model's max length\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class and its score\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_score = probabilities[0, predicted_class].item()\n",
    "\n",
    "    # Label mapping\n",
    "    label_map = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT ENOUGH INFO\"}  # Updated label mapping\n",
    "\n",
    "    return label_map[predicted_class], predicted_score  # Return label and score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silly example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "label, score = NLI(\"The sky is blue.\", \"The color of the pineapple is orange.\")\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, score = NLI(\n",
    "    \"The sky is light blue.\", # evidence\n",
    "    \"The color of the sky is blue.\" # claim\n",
    "    )\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Lets try for our original examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in evidence_claim_pairs:\n",
    "    print(\n",
    "        NLI(pair['evidence'],pair['claim']))\n",
    "    print(\"ground truth:\", pair['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Yay! \n",
    "\n",
    "Lets move onto working this for value kaleidescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Kaleidoscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangentially, if we want the original VP dataset. Otherwise skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download this, you need a HuggingFace access token. You should add this by running `huggingface-cli login` in the command line before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"hf://datasets/allenai/ValuePrism/full/full.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating VK experiment\n",
    "\n",
    "In the paper, they describe the experimental setup as:\n",
    "\n",
    "Concretely, for an LLM response with $n$ sentences $S = \\{s_1, · · · , s_n\\}$\n",
    "and VK’s explanation $e$ of how this value is related to the given situation, we calculate\n",
    "$$\n",
    "\\max^n_{i=1} \\mathbb{1}(NLI(s_i, e) \\textit{ is most\\_probable})\n",
    "$$\n",
    "as whether the value is reflected somewhere in\n",
    "the LLM’s response, with $\\mathbb{1}$ as the indicator\n",
    "function, NLI produces the entailment\n",
    "score, and $\\textit{most\\_probable}$ indicates that\n",
    "entailment is the most likely in the three-way\n",
    "classification (contradiction, entailment,\n",
    "neutral). The scores are then averaged across\n",
    "all values associated with each situation and\n",
    "then across situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, you loop over each LLM sentence, see whether it entails the value (explanation sentence), and if any of them do, the LLM response gets a score of 1 for that value and 0 otherwise. \n",
    "\n",
    "Each VP situation contains several values, so to score an LLM response for a situation, we average the number of values present over the total number of values for that situation. \n",
    "\n",
    "Then we can average over all situations to get a final score for this LLM on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per Tobin's request, we will also be storing which sentence(s) correspond to which value(s) so that we can do span metrics later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Responses for ValuePrism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.read_csv('data/questions_and_human_perspectives_with_responses.csv')\n",
    "vp = vp[vp.source=='valueprism']\n",
    "vp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Use NLTK's sent_tokenize to split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def find_span_indices(string, substring):\n",
    "    start_index = string.find(substring)\n",
    "    if start_index == -1:\n",
    "        return None  # Return None if the substring is not found\n",
    "    end_index = start_index + len(substring)\n",
    "    return (start_index, end_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment loop\n",
    "Let's just do gpt-4o-mini for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results json structure:\n",
    "\n",
    "```json\n",
    "rj = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"<question1_text>\": {\n",
    "            \"model_response\": \"<model_response>\",\n",
    "            \"values\": { # dict of all explanations for each value for the given question and the results\n",
    "                \"<explanation1_text>\": {\n",
    "                    \"labels\": [], # list of predicted labels ('support' 'refute' 'nei') for each sentence in model_response\n",
    "                    \"scores\": [], # list of scores of each predicted label for each sentence in model_response\n",
    "                    \"spans\": [] # list of tuples of the span of each sentence in model_response \n",
    "                },\n",
    "                ...\n",
    "                # rest of explanations\n",
    "            },\n",
    "            \"avg\": \"<avg score over the values>\" # avg num values present over the total num values for this question\n",
    "        },\n",
    "        ...\n",
    "        # rest of questions\n",
    "    },\n",
    "    ...\n",
    "    # (eventually) rest of models\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "# models = ['gpt-4o-mini','gpt-3.5-turbo','gemini-1.5-flash-002','mistral-7b-instruct','gemma-2-2b-it','llama-3.1-8B-it']\n",
    "# rj = {model: {} for model in models}\n",
    "results = {}\n",
    "\n",
    "\n",
    "# just doing gpt-4o-mini for now\n",
    "for id, row in vp.iterrows():\n",
    "    question = row.question\n",
    "    perspectives = ast.literal_eval(row.perspectives) # list of str of the form \"Value: <value>\\nExplanation: <explanation>\"\n",
    "    # we just want the explanation\n",
    "    explanations = [p.split(\"Explanation: \")[-1] for p in perspectives]\n",
    "    model_response = row['gpt-4o-mini']\n",
    "    results[question] = {\n",
    "        \"model_response\": model_response,\n",
    "        \"values\": {},\n",
    "        \"avg\": 0\n",
    "    }\n",
    "    S = tokenize_sentences(model_response) # sentences of LLM response S = {s_1, ... , s_n}\n",
    "    presence = [] # list to store the binary indicator function results\n",
    "    for e in explanations:\n",
    "        results[question][\"values\"][e] = {\n",
    "            \"labels\": [], \n",
    "            \"scores\": [],\n",
    "            \"spans\": []\n",
    "        }\n",
    "        for si in S:\n",
    "            label, score = NLI(si,e)\n",
    "            span = find_span_indices(model_response,si)\n",
    "            results[question][\"values\"][e][\"labels\"].append(label)\n",
    "            results[question][\"values\"][e][\"scores\"].append(score)\n",
    "            results[question][\"values\"][e][\"spans\"].append(span) \n",
    "        presence.append(1 if \"SUPPORTS\" in results[question][\"values\"][e][\"labels\"] else 0)\n",
    "    # now lets calcualte the average over all the values\n",
    "    results[question][\"avg\"] = np.mean(presence)\n",
    "    \n",
    "    # save results\n",
    "    with open('data/results/NLI_VP_results_gpt-4o-mini.json', 'w') as f:\n",
    "        json.dump(results,f,indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking on [MNLI](https://huggingface.co/datasets/nyu-mll/multi_nli)\n",
    "\n",
    "The model in the VitaminC paper was evaluated on MNLI (Williams et al., 2018), using \"the hypothesis as the\n",
    "claim and the premise as the evidence... on the “mismatched” evaluation set.\" The paper reports an accuracy of  78.89%.\n",
    "\n",
    "The labels are 0: entails, 1: neutral and 2: contradiction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "# Download the test split \n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation_matched': 'data/validation_matched-00000-of-00001.parquet', 'validation_mismatched': 'data/validation_mismatched-00000-of-00001.parquet'}\n",
    "df_mnli = pd.read_parquet(\"hf://datasets/nyu-mll/multi_nli/\" + splits[\"validation_mismatched\"])\n",
    "\n",
    "# label_mapping = {0: \"entails\", 1: \"neutral\", 2: \"contradiction\"} # true MNLI label mapping\n",
    "# label mapping to match the vitaminc labels\n",
    "label_mapping = {0: \"SUPPORTS\", 1: \"NOT ENOUGH INFO\", 2: \"REFUTES\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mnli.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one example and see if the model can predict it correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = df_mnli.sample(1)\n",
    "label_mapping[ex['label'].values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLI(ex['premise'].values[0],ex['hypothesis'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run baseline on sample of test set\n",
    "\n",
    "Lets sample 200 from the test set and predict on that to see whether we match the 78.89% accuracy from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "n = 200\n",
    "seed = 0\n",
    "sample_df_mnli = df_mnli.sample(n, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "import tqdm\n",
    "results = {}\n",
    "correct = 0\n",
    "for index, row in tqdm.tqdm(sample_df_mnli.iterrows(), total=n):\n",
    "    premise = row['premise']\n",
    "    hypothesis = row['hypothesis']\n",
    "    label = label_mapping[row[\"label\"]]\n",
    "    prediction, score = NLI(premise, hypothesis)\n",
    "    results[index] = {\n",
    "        \"promptID\": row[\"promptID\"],\n",
    "        \"pairID\": row[\"pairID\"],\n",
    "        \"premise\": premise,\n",
    "        \"hypothesis\": hypothesis,\n",
    "        \"prediction\": prediction,\n",
    "        \"score\": score,\n",
    "        \"label\": label\n",
    "    }\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "print(f\"Accuracy on MNLI sample size n={n} is {round(correct/n,3)*100}%\")\n",
    "\n",
    "with open(f'data/results/MNLI_predictions_seed-{seed}_n-{n}.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above prints \"Accuracy on MNLI sample size n=200 is 73.5%\" which is great! Yay!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres a lil function to calculate the accuracy when loading from a previously saved json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a src/entailment_vitc.py \n",
    "\n",
    "def process_results_json(file_path: str) -> float:\n",
    "    with open(file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    n = len(results)  # Calculate n based on the length of the results json\n",
    "    correct = sum(1 for result in results.values() if result['prediction'] == result['label'])\n",
    "    accuracy = round(correct / n, 3) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "process_results_json('data/results/MNLI_predictions_seed-0_n-200.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
