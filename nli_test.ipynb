{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out NLI \n",
    "\n",
    "Notebook to get the baseline usage of the model `tals/albert-xlarge-vitaminc` from [HuggingFace](https://huggingface.co/tals/albert-xlarge-vitaminc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few text examples for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_claim_pairs = [\n",
    "    {\n",
    "        \"evidence\": \"The new policy has led to a significant decrease in crime rates.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Supports\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"There are no studies showing a direct link between the policy and crime rates.\",\n",
    "        \"claim\": \"The new policy has a high impact on crime rates.\",\n",
    "        \"label\": \"Not enough info\"\n",
    "    },\n",
    "    {\n",
    "        \"evidence\": \"Crime rates have increased since the policy was implemented.\",\n",
    "        \"claim\": \"The new policy reduces crime.\",\n",
    "        \"label\": \"Refutes\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tals/albert-xlarge-vitaminc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction pipeline\n",
    "\n",
    "Takes in two strings, evidence and claim. Outputs the tuple of the label and corresponding prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLI(text_a, text_b):\n",
    "    \"\"\"Predicts the relationship between a claim and evidence. \n",
    "\n",
    "    Args:\n",
    "        text_a (str): The evidence statement.\n",
    "        text_b (str): The claim statement.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted relationship label (\"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\").\n",
    "        float: The confidence score of the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        text_a, text_b,\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        padding=True,         # Pad to the longest sequence in the batch\n",
    "        truncation=True       # Truncate to the model's max length\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get the predicted class and its score\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    predicted_score = probabilities[0, predicted_class].item()\n",
    "\n",
    "    # Label mapping\n",
    "    label_map = {0: \"SUPPORTS\", 1: \"REFUTES\", 2: \"NOT ENOUGH INFO\"}  # Updated label mapping\n",
    "\n",
    "    return label_map[predicted_class], predicted_score  # Return label and score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silly example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "label, score = NLI(\"The sky is blue.\", \"The color of the pineapple is orange.\")\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, score = NLI(\n",
    "    \"The sky is light blue.\", # evidence\n",
    "    \"The color of the sky is blue.\" # claim\n",
    "    )\n",
    "print(f\"Predicted class: {label}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Lets try for our original examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in evidence_claim_pairs:\n",
    "    print(\n",
    "        NLI(pair['evidence'],pair['claim']))\n",
    "    print(\"ground truth:\", pair['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Yay! \n",
    "\n",
    "Lets move onto working this for value kaleidescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Kaleidoscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangentially, if we want the original VP dataset. Otherwise skip this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download this, you need a HuggingFace access token. You should add this by running `huggingface-cli login` in the command line before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"hf://datasets/allenai/ValuePrism/full/full.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating VK experiment\n",
    "\n",
    "In the paper, they describe the experimental setup as:\n",
    "\n",
    "Concretely, for an LLM response with $n$ sentences $S = \\{s_1, · · · , s_n\\}$\n",
    "and VK’s explanation $e$ of how this value is related to the given situation, we calculate\n",
    "$$\n",
    "\\max^n_{i=1} \\mathbb{1}(NLI(s_i, e) \\textit{ is most\\_probable})\n",
    "$$\n",
    "as whether the value is reflected somewhere in\n",
    "the LLM’s response, with $\\mathbb{1}$ as the indicator\n",
    "function, NLI produces the entailment\n",
    "score, and $\\textit{most\\_probable}$ indicates that\n",
    "entailment is the most likely in the three-way\n",
    "classification (contradiction, entailment,\n",
    "neutral). The scores are then averaged across\n",
    "all values associated with each situation and\n",
    "then across situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, you loop over each LLM sentence, see whether it entails the value (explanation sentence), and if any of them do, the LLM response gets a score of 1 for that value and 0 otherwise. \n",
    "\n",
    "Each VP situation contains several values, so to score an LLM response for a situation, we average the number of values present over the total number of values for that situation. \n",
    "\n",
    "Then we can average over all situations to get a final score for this LLM on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per Tobin's request, we will also be storing which sentence(s) correspond to which value(s) so that we can do span metrics later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Responses for ValuePrism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = pd.read_csv('data/questions_and_human_perspectives_with_responses.csv')\n",
    "vp = vp[vp.source=='valueprism']\n",
    "vp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Use NLTK's sent_tokenize to split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def find_span_indices(string, substring):\n",
    "    start_index = string.find(substring)\n",
    "    if start_index == -1:\n",
    "        return None  # Return None if the substring is not found\n",
    "    end_index = start_index + len(substring)\n",
    "    return (start_index, end_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment loop\n",
    "Let's just do gpt-4o-mini for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results json structure:\n",
    "\n",
    "```json\n",
    "rj = {\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"<question1_text>\": {\n",
    "            \"model_response\": \"<model_response>\",\n",
    "            \"values\": { # dict of all explanations for each value for the given question and the results\n",
    "                \"<explanation1_text>\": {\n",
    "                    \"labels\": [], # list of predicted labels ('support' 'refute' 'nei') for each sentence in model_response\n",
    "                    \"scores\": [], # list of scores of each predicted label for each sentence in model_response\n",
    "                    \"spans\": [] # list of tuples of the span of each sentence in model_response \n",
    "                },\n",
    "                ...\n",
    "                # rest of explanations\n",
    "            },\n",
    "            \"avg\": \"<avg score over the values>\" # avg num values present over the total num values for this question\n",
    "        },\n",
    "        ...\n",
    "        # rest of questions\n",
    "    },\n",
    "    ...\n",
    "    # (eventually) rest of models\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "# models = ['gpt-4o-mini','gpt-3.5-turbo','gemini-1.5-flash-002','mistral-7b-instruct','gemma-2-2b-it','llama-3.1-8B-it']\n",
    "# rj = {model: {} for model in models}\n",
    "results = {}\n",
    "\n",
    "\n",
    "# just doing gpt-4o-mini for now\n",
    "for id, row in vp.iterrows():\n",
    "    question = row.question\n",
    "    perspectives = ast.literal_eval(row.perspectives) # list of str of the form \"Value: <value>\\nExplanation: <explanation>\"\n",
    "    # we just want the explanation\n",
    "    explanations = [p.split(\"Explanation: \")[-1] for p in perspectives]\n",
    "    model_response = row['gpt-4o-mini']\n",
    "    results[question] = {\n",
    "        \"model_response\": model_response,\n",
    "        \"values\": {},\n",
    "        \"avg\": 0\n",
    "    }\n",
    "    S = tokenize_sentences(model_response) # sentences of LLM response S = {s_1, ... , s_n}\n",
    "    presence = [] # list to store the binary indicator function results\n",
    "    for e in explanations:\n",
    "        results[question][\"values\"][e] = {\n",
    "            \"labels\": [], \n",
    "            \"scores\": [],\n",
    "            \"spans\": []\n",
    "        }\n",
    "        for si in S:\n",
    "            label, score = NLI(si,e)\n",
    "            span = find_span_indices(model_response,si)\n",
    "            results[question][\"values\"][e][\"labels\"].append(label)\n",
    "            results[question][\"values\"][e][\"scores\"].append(score)\n",
    "            results[question][\"values\"][e][\"spans\"].append(span) \n",
    "        presence.append(1 if \"SUPPORTS\" in results[question][\"values\"][e][\"labels\"] else 0)\n",
    "    # now lets calcualte the average over all the values\n",
    "    results[question][\"avg\"] = np.mean(presence)\n",
    "    \n",
    "    # save results\n",
    "    with open('data/results/NLI_VP_results_got-4o-mini.json', 'w') as f:\n",
    "        json.dump(results,f,indent=2)\n",
    "            \n",
    "    \n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
