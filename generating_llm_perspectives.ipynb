{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating LLM Perspectives & Comparing to Human Perspectives\n",
    "So far, we've been using the human provided responses as the \"ground truth\" for what is the full overton window. However, we can also generate our own \"ground truth\" by asking LLMs to provide their own perspectives.\n",
    "\n",
    "We're then going to compare the mapping of the LLM perspectives to the human provided responses.\n",
    "\n",
    "This takes in the `habermas_machine_questions_with_responses.csv` file and generates responses from the LLMs and will update it with the new LLM perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')\n",
    "\n",
    "df_questions = pd.read_csv(DATA_PATH+'habermas_machine_questions_with_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Generation of LLM Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "class SinglePerspective(BaseModel):\n",
    "    perspective: str\n",
    "\n",
    "class PerspectiveChain(BaseModel):\n",
    "    steps: list[SinglePerspective]\n",
    "\n",
    "def generate_perspectives(question: str):\n",
    "    \"\"\"\n",
    "    Generate a multiple perspectives of answers to a question.\n",
    "\n",
    "    Args:\n",
    "        question: Context question or query\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful assistant that generates multiple perspectives of answers to a question. You will be given a question and you will generate a list of possible answer perspectives. Make sure you cover all possible perspectives but do not repeat yourself.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "Now, step by step, outline each broad answer perspective to this question.\"\"\"\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=1,\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"PerspectiveChain\", \n",
    "                \"schema\": PerspectiveChain.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "\n",
    "    result_object = json.loads(chat_response.choices[0].message.content)\n",
    "    return [step['perspective'] for step in result_object['steps']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Using a loop rather than .apply for restart simplicity & tqdm\n",
    "llm_perspectives = []\n",
    "for question in tqdm(df_questions['question.text'], desc=\"Generating perspectives\"):\n",
    "    perspectives = generate_perspectives(question)\n",
    "    llm_perspectives.append(perspectives)\n",
    "\n",
    "df_questions['llm_perspectives'] = llm_perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_perspectives = df_questions[['question.text', 'question_topic', 'question_id', 'llm_perspectives']]\n",
    "df_llm_perspectives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep things separate and clean, we're going to save these to a different file.\n",
    "df_llm_perspectives.to_csv(DATA_PATH+'habermas_machine_questions_with_LLM_generated_perspectives.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df_llm_perspectives = pd.read_csv(DATA_PATH+'habermas_machine_questions_with_LLM_generated_perspectives.csv')\n",
    "df_questions = pd.read_csv(DATA_PATH+'habermas_machine_questions_with_responses.csv')\n",
    "df_questions['own_opinion.text'] = df_questions['own_opinion.text'].apply(ast.literal_eval)\n",
    "df_llm_perspectives['llm_perspectives'] = df_llm_perspectives['llm_perspectives'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the df_llm_perspectives and df_questions by question_id\n",
    "df_llm_perspectives = df_llm_perspectives.merge(df_questions[['question_id', 'own_opinion.text']], on='question_id', how='left')\n",
    "df_llm_perspectives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def are_these_perspectives_the_same(perspective_a: str, perspective_b: str):\n",
    "    \"\"\"\n",
    "    Determine if two perspectives are the same.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"You will be given two perspectives and you will determine if they are the same. Read carefully the two perspectives and answer yes if they are expressing the same broad perspective or opinion. Answer no otherwise. ONLY say a single word: 'yes' or 'no'.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Perspective A: {perspective_a}\n",
    "Perspective B: {perspective_b}\n",
    "Are these the same perspective/opinion? Yes/no answer:\"\"\"\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=1\n",
    "    )\n",
    "    return 1 if chat_response.choices[0].message.content.strip().lower() == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_perspective_matrix = [[0]*len(row['own_opinion.text'])]*len(row['llm_perspectives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_perspective_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_llm_perspectives.iterrows():\n",
    "    print(row['question.text'])\n",
    "    print(row['own_opinion.text'])\n",
    "    # We're gonna generate a pairwise comparison of the own opinion and each of the LLM perspectives.\n",
    "    # The complexity of this is pretty bad given the size of the context windows and can be very slow (20s per question).\n",
    "    same_perspective_matrix = [[0]*len(row['own_opinion.text'])]*len(row['llm_perspectives'])\n",
    "    for i, perspective in enumerate(row['llm_perspectives']):\n",
    "        for j, own_opinion in enumerate(row['own_opinion.text']):\n",
    "            same_perspective_matrix[i][j] = are_these_perspectives_the_same(perspective, own_opinion)\n",
    "    print(same_perspective_matrix)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['llm_perspectives'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row['own_opinion.text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP Notes\n",
    "\n",
    "It's really hard to actually determine if two perspectives are the same. We're going to need to try to interate on this.\n",
    "\n",
    "In addition, using the LLM to do so is reasonable slow (but not too bad). Perhaps a clustering method would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
