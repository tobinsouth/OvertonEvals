{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGZKXwtiiBiL"
   },
   "source": [
    "# Habermas Machine data and preprocessing\n",
    "This follows the instructions in the [Habermas Machine github repo](https://github.com/google-deepmind/habermas_machine), modified to save the processed questions and opinions.\n",
    "\n",
    "The input to this notebook is the Habermas Machine work, and the output is a dataframe with the questions and opinions `habermas_machine_questions.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-21EkbgjiDtF"
   },
   "source": [
    "# Setup and Importing Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Bou7ACLh6Yj"
   },
   "outputs": [],
   "source": [
    "# Clone github repo locally.\n",
    "!git clone https://github.com/google-deepmind/habermas_machine\n",
    "\n",
    "# Adjust path.\n",
    "import sys\n",
    "sys.path.insert(0,'/content/habermas_machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdDwJRN2iGtW"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import ast\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Local imports\n",
    "from habermas_machine.analysis import live_loading, serialise, types\n",
    "\n",
    "# Load helper keys used with dataframes.\n",
    "DFKeys = serialise.SerialisedComparisonKeys\n",
    "DFGroupedKeys = serialise.GroupedSerialisedComparisonKeys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDRqfi3MiI5x"
   },
   "outputs": [],
   "source": [
    "#@title Load all comparison data from Google Cloud Storage.\n",
    "comparison_data_location = (\n",
    "    'https://storage.googleapis.com/habermas_machine/datasets/hm_all_candidate_comparisons.parquet'\n",
    ")\n",
    "response = requests.get(comparison_data_location)\n",
    "with io.BytesIO(response.content) as f:\n",
    "  df_all = pd.read_parquet(f)\n",
    "clear_output()\n",
    "\n",
    "df_all.shape # Shape of full comparison data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdBIn4jgiL7z"
   },
   "outputs": [],
   "source": [
    "#@title Explore data\n",
    "\n",
    "print(\"Number of participant sessions (before pre-processing):\",\n",
    "      df_all[DFKeys.COMPARISON_PARTICIPANT_ID].nunique())\n",
    "\n",
    "print(\n",
    "    \"Number of participant sessions (before pre-processing) of each collection:\",\n",
    "    df_all[\n",
    "        [DFKeys.COMPARISON_VERSION, DFKeys.COMPARISON_PARTICIPANT_ID]\n",
    "    ].drop_duplicates().groupby(DFKeys.COMPARISON_VERSION).count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WlV_Ye1iOwa"
   },
   "source": [
    "# Example preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJjl6SLZiNYy"
   },
   "outputs": [],
   "source": [
    "#@title Select dataset\n",
    "\n",
    "dataset_name = 'training' # @param [\"training\", \"cohort1_ablation_iid_v1\", \"cohort2_ablation_iid_v2\", \"cohort3_ablation_ood_v1\", 'cohort4_critique_exclusion', 'cohort5_opinion_exposure', 'cohort6_human_mediator', 'virtual_citizens_assembly']\n",
    "\n",
    "# Set dataset and parameters based on dataset_name.\n",
    "if dataset_name == 'training':\n",
    "  df = df_all[\n",
    "      df_all[DFKeys.COMPARISON_VERSION].isin([\n",
    "          'TRAINING_DATA_V1',\n",
    "          'TRAINING_DATA_V2',\n",
    "          'TRAINING_DATA_V3',\n",
    "          'TRAINING_DATA_V4',\n",
    "          'TRAINING_DATA_V5',\n",
    "      ])\n",
    "  ]\n",
    "  # Backwards incompatibility issue with training data.\n",
    "  df = df.drop(columns=[\n",
    "      DFKeys.CANDIDATES_ALL_REWARD_DATA_WELFARE_OR_RANK,\n",
    "      DFKeys.CANDIDATES_REWARD_DATA_WELFARE_OR_RANK,\n",
    "  ])\n",
    "  min_size_parameters = None\n",
    "  remove_groups_with_repeat_participants = False\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
    "elif dataset_name == 'cohort1_ablation_iid_v1':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT1_ABLATION_IID_V1']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V1\n",
    "  remove_groups_with_repeat_participants = True\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
    "elif dataset_name == 'cohort2_ablation_iid_v2':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT2_ABLATION_IID_V2']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V2\n",
    "  remove_groups_with_repeat_participants = True\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
    "elif dataset_name == 'cohort3_ablation_ood_v1':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT3_ABLATION_OOD_V1']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_OOD_V1\n",
    "  remove_groups_with_repeat_participants = True\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
    "elif dataset_name == 'cohort4_critique_exclusion':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT4_CRITIQUE_EXCLUSION']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_CRITIQUE_EXCLUSION\n",
    "  remove_groups_with_repeat_participants = True\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
    "elif dataset_name == 'cohort5_opinion_exposure':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT5_OPINION_EXPOSURE']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_OPINION_EXPOSURE\n",
    "  remove_groups_with_repeat_participants = False\n",
    "  valid_candidate_provenances = (\n",
    "      types.ResponseProvenance.HUMAN_CITIZEN, # Candidates are other opinions.\n",
    "  )\n",
    "elif dataset_name == 'cohort6_human_mediator':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT6_HUMAN_MEDIATOR']\n",
    "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_HUMAN_MEDIATOR\n",
    "  remove_groups_with_repeat_participants = False\n",
    "  # Candidates can be either model or human statements.\n",
    "  valid_candidate_provenances = (\n",
    "        types.ResponseProvenance.MODEL_MEDIATOR,\n",
    "        types.ResponseProvenance.HUMAN_MEDIATOR, )\n",
    "elif dataset_name == 'virtual_citizens_assembly':\n",
    "  df = df_all[df_all[DFKeys.COMPARISON_VERSION].isin([\n",
    "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK3',\n",
    "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK4',\n",
    "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK5'\n",
    "  ])]\n",
    "  min_size_parameters = None\n",
    "  remove_groups_with_repeat_participants = False\n",
    "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G3Ujd2niS6A"
   },
   "outputs": [],
   "source": [
    "#@title Example pre-processing\n",
    "print('processing', dataset_name)\n",
    "print('original df shape', df.shape)\n",
    "live_loading.check_consistent_tuple_lengths_in_grouped_columns(\n",
    "    df, groups_columns=[\n",
    "        DFGroupedKeys.OTHER_OPINIONS, DFGroupedKeys.CANDIDATES])\n",
    "\n",
    "# First, unnest columns (e.g., ratings of statements).\n",
    "df_unnested = live_loading.unnest_nested_columns(df)\n",
    "print('unnested df shape', df_unnested.shape)\n",
    "\n",
    "# Remove rows where OWN_OPINION is not HUMAN_CITIZEN (e.g., MOCKs).\n",
    "df_unnested = live_loading.filter_on_response_provenances(\n",
    "    df_unnested,\n",
    "    provenance_column=DFKeys.OWN_OPINION_PROVENANCE,\n",
    "    valid_provenances=(types.ResponseProvenance.HUMAN_CITIZEN,),\n",
    ")\n",
    "print('filtered df shape after removing invalid opinions', df_unnested.shape)\n",
    "\n",
    "# Remove rows where CANDIDATES_PROVEANACE is not as expected:\n",
    "# MODEL_MEDIATOR for most data sets. Can also be HUMAN_CITIZEN or HUMAN_MEDIATOR\n",
    "# for opinion exposure and human mediator comparison, respectively.\n",
    "df_unnested = live_loading.filter_on_response_provenances(\n",
    "    df_unnested,\n",
    "    provenance_column=DFKeys.CANDIDATES_PROVENANCE,\n",
    "    valid_provenances=valid_candidate_provenances,\n",
    ")\n",
    "print('filtered df shape after removing invalid candidates', df_unnested.shape)\n",
    "\n",
    "# Remove mock ratings.\n",
    "df_unnested = live_loading.filter_out_mock_ratings(\n",
    "    df_unnested, rating_type=live_loading.RatingTypes.AGREEMENT)\n",
    "print('filtered df shape after removing mock ratings', df_unnested.shape)\n",
    "\n",
    "# Remove mock rankings.\n",
    "df_unnested = live_loading.filter_out_mock_rankings(df_unnested)\n",
    "print('filtered df shape after removing mock rankings', df_unnested.shape)\n",
    "\n",
    "# Add a column with the numerical equivalents for the Likerts.\n",
    "df_unnested = live_loading.add_numerical_ratings(df_unnested)\n",
    "print('added numerical ratings df shape', df_unnested.shape)\n",
    "\n",
    "# Optional (not used in training or human mediator eval):\n",
    "# Remove groups with repeat participants.\n",
    "if remove_groups_with_repeat_participants:\n",
    "  df_unnested = live_loading.filter_groups_with_repeat_participants(\n",
    "      df_unnested, 'worker_id')\n",
    "  print('filtered df after removing groups with repeat participants', df_unnested.shape)\n",
    "\n",
    "# Renest previously unnested columns.\n",
    "df_nested = live_loading.nest_columns_as_tuples(df_unnested)\n",
    "print('renested df shape', df_nested.shape)\n",
    "\n",
    "# Human Mediator specific preprocessing: Only keep rounds where both human and\n",
    "# model generated statements.\n",
    "if dataset_name == 'cohort6_human_mediator':\n",
    "  df_nested = df_nested[\n",
    "      df_nested[DFKeys.CANDIDATES_PROVENANCE].apply(len) == 2\n",
    "  ]\n",
    "  print('only keeping rounds where both human and model made statement',\n",
    "        df_nested.shape)\n",
    "\n",
    "# Optional: Filter by number of groups of min size (pre-registration criteria).\n",
    "# Note, this should be applied to only a single evaluation dataset and not\n",
    "# multiple datasets at the same time.\n",
    "if min_size_parameters is not None:\n",
    "  df_nested = live_loading.filter_by_number_of_groups_of_min_size(\n",
    "      df_nested,\n",
    "      **min_size_parameters.value)\n",
    "  print('filtered df after setting number of groups of min size', df_nested.shape)\n",
    "\n",
    "print('Number of groups in preprocessed dataframe:',\n",
    "      df_nested[DFKeys.LAUNCH_ID].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be helpful for debugging.\n",
    "pd.set_option('display.max_columns', 400)  # Show all columns\n",
    "pd.set_option('display.max_rows', 400)     # Show all rows\n",
    "\n",
    "# This is an example of what the data looks like.\n",
    "df_nested.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_nested[df_nested['iteration_index'] == 0] # This stops us having duplicates from multiple rounds.\n",
    "\n",
    "df_questions = df_questions[['question.text','question.topic','own_opinion.text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "df_questions[df_questions['question.text'] == 'Are human beings the most intelligent life form in the Universe?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_question_to_opinions = df_questions.groupby('question.text')['own_opinion.text']\n",
    "    \n",
    "# Let's check that each question has no duplicates.\n",
    "assert (grouped_question_to_opinions.apply(lambda x: x.count() / x.drop_duplicates().count()) == 1).all()\n",
    "\n",
    "assert (df_questions.groupby('question.text')['question.topic'].nunique() == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opinions_wrapped = df_questions.groupby('question.text')['own_opinion.text'].apply(lambda x: x.to_list()).to_frame().reset_index()\n",
    "\n",
    "question_to_topic = df_questions.set_index('question.text')['question.topic'].to_dict()\n",
    "\n",
    "df_opinions_wrapped['question_topic'] = df_opinions_wrapped['question.text'].map(question_to_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some unique question ids to make life easier later.\n",
    "df_opinions_wrapped['question_id'] = df_opinions_wrapped.groupby('question.text').ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df_opinions_wrapped.to_csv('data/habermas_machine_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
