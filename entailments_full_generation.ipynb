{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the entailment methods on the full dataset\n",
    "\n",
    "This notebook takes the lessons from the `comparing_entailment_methods.ipynb` notebook and runs the final entailment methods on the full dataset to generate metrics for the different LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "import openai\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.entailment import entailment_from_gpt_json, process_entailment_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('data/habermas_machine_questions_with_responses.csv')\n",
    "df_questions['own_opinion.text'] = df_questions['own_opinion.text'].apply(ast.literal_eval)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should make sure these models are up to date before running this.\n",
    "response_models = ['gpt-3.5-turbo', 'gpt-4o', 'llama-3.1-8B', 'mistral-7B'] \n",
    "assert all([model in df_questions.columns for model in response_models]) # Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_model= 'gpt-4o-mini'\n",
    "entailment_results = []\n",
    "for _, row in tqdm(df_questions.iterrows(), total=df_questions.shape[0], desc=\"Questions\", leave=True):\n",
    "    question = row['question.text']\n",
    "    question_id = row['question_id']\n",
    "    opinions = row['own_opinion.text']\n",
    "    with tqdm(total=len(opinions), desc=\"Opinions\", leave=False) as opinion_bar:\n",
    "        for opinion_idx, opinion in enumerate(opinions):\n",
    "            # with tqdm(total=len(response_models), desc=\"Response Models\", leave=False) as response_bar:\n",
    "            for response_model in response_models:\n",
    "                response = row[response_model]\n",
    "                entailment_result = entailment_from_gpt_json(question, response, opinion, model='gpt-4o-mini')\n",
    "                matches = process_entailment_result(entailment_result, response)\n",
    "                entailment_results.append({\n",
    "                    'question_id': question_id,\n",
    "                    'opinion_idx': opinion_idx,\n",
    "                    'response_model': response_model,\n",
    "                    'entailment_model': entailment_model,\n",
    "                    'entailment_result': entailment_result,\n",
    "                    'matches': matches\n",
    "                })\n",
    "                # response_bar.update(1)\n",
    "            opinion_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_entailment_results = pd.DataFrame(entailment_results)\n",
    "raw_entailment_results.to_csv('data/raw_entailment_results.csv', index=False)\n",
    "raw_entailment_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating metrics\n",
    "\n",
    "Here we're going to generate a sample of metrics over the entailment results for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_entailment_results['match_total_length'] = raw_entailment_results['matches'].apply(\n",
    "    lambda x: sum([match[1]-match[0] for match in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_entailment_results['question_length'] = raw_entailment_results['question_id'].map(df_questions.set_index('question_id')['question.text'].str.len())\n",
    "raw_entailment_results['match_length_ratio'] = raw_entailment_results['match_total_length'] / raw_entailment_results['question_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_entailment_results.groupby(['response_model'])['match_length_ratio'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now seaborn heatmap plot the correlation between the entailment length correlation across pairs of models\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pivot the data to create a matrix of match lengths by question/opinion and model\n",
    "pivot_df = raw_entailment_results.pivot_table(\n",
    "    index=['question_id', 'opinion_idx'],\n",
    "    columns='response_model',\n",
    "    values='match_length_ratio'\n",
    ")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = pivot_df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,  # Show correlation values\n",
    "    cmap='coolwarm',  # Color scheme\n",
    "    vmin=-1, vmax=1,  # Force scale from -1 to 1\n",
    "    center=0,  # Center the colormap at 0\n",
    "    square=True  # Make cells square\n",
    ")\n",
    "plt.title('Correlation of Entailment Lengths Between Models')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
